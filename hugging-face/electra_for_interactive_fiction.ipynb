{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOo1PZfJl7Es",
    "outputId": "2aafda66-1faf-4f52-a155-402ee3432857",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: tokenizers in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (0.11.6)\n",
      "Requirement already satisfied: datasets in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (1.18.4)\n",
      "Requirement already satisfied: seqeval in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (1.21.3)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.0.47)\n",
      "Requirement already satisfied: requests in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (2021.11.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: protobuf in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (3.19.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.1.96)\n",
      "Requirement already satisfied: multiprocess in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: aiohttp in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from datasets) (2021.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from seqeval) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (1.26.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: click in /anaconda/envs/py38_pytorch/lib/python3.8/site-packages (from sacremoses->transformers[sentencepiece]) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece] tokenizers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2Wiz9NCuHx7z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoModelForTokenClassification, AutoTokenizer, DataCollatorWithPadding, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "checkpoint = \"google/electra-small-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502,
     "referenced_widgets": [
      "aac2139debf34899ac3d922c1f75844b",
      "3490cfd5cd7c4b22be6af705a654ab95",
      "f7b6d8f6a01f4f78bac63de8cdf2e27e",
      "7b29986357944df1bf80b25fdf3e3508",
      "b8c3222ec9dc4672bbf33f090784a44b",
      "d90923208f7446b7b71715ce4f80b924",
      "2518bc84617042e890c1300763303fca",
      "7cb679855b374ff9847606cea3c83f43",
      "b2907cabe1454e068a522fd4e4dab891",
      "c1f5cab961dd44a0b29e1000665c3d5d",
      "2192c7a7132244eba01c1de17e02c3a6"
     ]
    },
    "id": "dgHIOtQ-j_Gj",
    "outputId": "e4a6d5f6-70e4-4ab4-9165-8219d099a543"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/ap_default/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef5c6f0b0b848fa9d4a8daee8502e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_list = load_dataset(\"conll2003\")[\"train\"].features[\"chunk_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7f8c5f914cfc28ef\n",
      "Reusing dataset csv (/home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf9323e85c44a61a9c75508f987d30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-d139fe2704a766fb.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-7fea2cb9ec57fcc6.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-6e16171400876ed3.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-c129e8c0ca276c9c.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-becc72b9ae63cb38.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-ffd3e58143e5923b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 3329\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 820\n",
      "    })\n",
      "    full: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 4149\n",
      "    })\n",
      "})\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=54, names=['112', '1132', '1210', '1262', '139', '1390', '16', '160', '1688', '17', '171', '197', '2135', '2161', '254', '263', '264', '265', '273', '276', '279', '2824', '283', '289', '293', '301', '31', '352', '395', '40', '41', '410', '414', '416', '42', '424', '43', '54', '55', '56', '59', '590', '6', '62', '63', '64', '65', '652', '66', '683', '7', '770', '801', 'unknown'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=54, names=['112', '1132', '1210', '1262', '139', '1390', '16', '160', '1688', '17', '171', '197', '2135', '2161', '254', '263', '264', '265', '273', '276', '279', '2824', '283', '289', '293', '301', '31', '352', '395', '40', '41', '410', '414', '416', '42', '424', '43', '54', '55', '56', '59', '590', '6', '62', '63', '64', '65', '652', '66', '683', '7', '770', '801', 'unknown'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': 1, 'label': 31, 'sentence': 'get green'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b98ff5898fa6ba87\n",
      "Reusing dataset csv (/home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e08129c5d6143cb92cbf6de5515e7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-5e4ea063706b6b77.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-daae674e05534f84.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-44db151f32c55575.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-9e357abff7ecfa54.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-caf14089f084b17d.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-f2ef2445113f669e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 3327\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 832\n",
      "    })\n",
      "    full: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 4159\n",
      "    })\n",
      "})\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=3, names=['0', '1', 'unknown'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=3, names=['0', '1', 'unknown'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': 1, 'label': 0, 'sentence': 'beetlebaum'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7581f73791510896\n",
      "Reusing dataset csv (/home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd72026235245d792b560fed4221693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-34a4bd749b78cf59.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-bea5bd4198cf5603.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-78ba92e3d9b3080a.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-4461d705c5909d1c.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-3c65c55cd04946c2.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-440d18e52fb8a1cf.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 4718\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 1154\n",
      "    })\n",
      "    full: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 5872\n",
      "    })\n",
      "})\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=70, names=['appear-48.1.1', 'assuming_position-50', 'banish-10.2', 'body_internal_states-40.6', 'break-45.1', 'bump-18.4', 'carry-11.4', 'carry-11.4-1-1', 'carve-21.2-2', 'chase-51.6', 'crane-40.3.2', 'cut-21.1-1', 'disassemble-23.3', 'eat-39.1-1', 'eat-39.1-2', 'escape-51.1-1', 'exist-47.1-1', 'feeding-39.7', 'fill-9.8', 'fulfilling-13.4.1', 'funnel-9.3-2-1', 'get-13.5.1', 'give-13.1-1', 'hit-18.1-1', 'hurt-40.8.3-2', 'investigate-35.4', 'knead-26.5', 'learn-14-1', 'learn-14-2-1', 'long-32.2-1', 'manner_speaking-37.3', 'modes_of_being_with_motion-47.3', 'murder-42.1-1', 'other_cos-45.4', 'peer-30.3', 'performance-26.7-1-1', 'pour-9.5', 'preparing-26.3-2', 'push-12-1', 'push-12-1-1', 'put-9.1-2', 'put_direction-9.4', 'put_spatial-9.2-1', 'remove-10.1', 'roll-51.3.1', 'run-51.3.2', 'say-37.7-1', 'scribble-25.2', 'search-35.2', 'see-30.1', 'see-30.1-1', 'shake-22.3-2', 'sight-30.2', 'simple_dressing-41.3.1', 'slide-11.2', 'slide-11.2-1-1', 'snooze-40.4', 'spatial_configuration-47.6', 'split-23.2', 'spray-9.7-1', 'stalk-35.3', 'substance_emission-43.4', 'talk-37.5', 'tape-22.4', 'throw-17.1-1-1', 'touch-20-1', 'transcribe-25.4', 'transfer_mesg-37.1.1', 'unknown', 'wipe_manner-10.4.1-1'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=70, names=['appear-48.1.1', 'assuming_position-50', 'banish-10.2', 'body_internal_states-40.6', 'break-45.1', 'bump-18.4', 'carry-11.4', 'carry-11.4-1-1', 'carve-21.2-2', 'chase-51.6', 'crane-40.3.2', 'cut-21.1-1', 'disassemble-23.3', 'eat-39.1-1', 'eat-39.1-2', 'escape-51.1-1', 'exist-47.1-1', 'feeding-39.7', 'fill-9.8', 'fulfilling-13.4.1', 'funnel-9.3-2-1', 'get-13.5.1', 'give-13.1-1', 'hit-18.1-1', 'hurt-40.8.3-2', 'investigate-35.4', 'knead-26.5', 'learn-14-1', 'learn-14-2-1', 'long-32.2-1', 'manner_speaking-37.3', 'modes_of_being_with_motion-47.3', 'murder-42.1-1', 'other_cos-45.4', 'peer-30.3', 'performance-26.7-1-1', 'pour-9.5', 'preparing-26.3-2', 'push-12-1', 'push-12-1-1', 'put-9.1-2', 'put_direction-9.4', 'put_spatial-9.2-1', 'remove-10.1', 'roll-51.3.1', 'run-51.3.2', 'say-37.7-1', 'scribble-25.2', 'search-35.2', 'see-30.1', 'see-30.1-1', 'shake-22.3-2', 'sight-30.2', 'simple_dressing-41.3.1', 'slide-11.2', 'slide-11.2-1-1', 'snooze-40.4', 'spatial_configuration-47.6', 'split-23.2', 'spray-9.7-1', 'stalk-35.3', 'substance_emission-43.4', 'talk-37.5', 'tape-22.4', 'throw-17.1-1-1', 'touch-20-1', 'transcribe-25.4', 'transfer_mesg-37.1.1', 'unknown', 'wipe_manner-10.4.1-1'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': 1, 'label': 3, 'sentence': 'shazok storm'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-dc77ba31d5024efe\n",
      "Reusing dataset csv (/home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc7cb7d43ba4797894c6cb79b483034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-1e39f50f01b28e67.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-8f2e5c046e0522f9.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-db29089dda3c7575.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-95e5e8c8172e7755.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-db837b05d09e5ec5.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-3d66ad711a645283.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 3138\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 758\n",
      "    })\n",
      "    full: Dataset({\n",
      "        features: ['idx', 'label', 'sentence'],\n",
      "        num_rows: 3896\n",
      "    })\n",
      "})\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=84, names=['answer.v.01', 'ask.v.01', 'ask.v.02', 'blow.v.01', 'brandish.v.01', 'break.v.05', 'burn.v.01', 'buy.v.01', 'charge.v.17', 'choose.v.01', 'clean.v.01', 'climb.v.01', 'close.v.01', 'connect.v.01', 'consult.v.02', 'cut.v.01', 'dig.v.01', 'drink.v.01', 'drive.v.01', 'drop.v.01', 'eat.v.01', 'enter.v.01', 'examine.v.02', 'exit.v.01', 'fill.v.01', 'follow.v.01', 'give.v.03', 'hit.v.02', 'hit.v.03', 'insert.v.01', 'insert.v.02', 'inventory.v.01', 'jump.v.01', 'kill.v.01', 'lie_down.v.01', 'light_up.v.05', 'listen.v.01', 'look.v.01', 'lower.v.01', 'memorize.v.01', 'move.v.02', 'note.v.04', 'open.v.01', 'play.v.03', 'pour.v.01', 'pray.v.01', 'press.v.01', 'pull.v.04', 'push.v.01', 'put.v.01', 'raise.v.02', 'read.v.01', 'remove.v.01', 'repeat.v.01', 'rub.v.01', 'say.v.08', 'search.v.04', 'sequence.n.02', 'set.v.05', 'shake.v.01', 'shoot.v.01', 'show.v.01', 'sit_down.v.01', 'skid.v.04', 'sleep.v.01', 'smash.v.02', 'smell.v.01', 'stand.v.03', 'switch_off.v.01', 'switch_on.v.01', 'take.v.04', 'take_off.v.06', 'talk.v.02', 'tell.v.03', 'throw.v.01', 'touch.v.01', 'travel.v.01', 'turn.v.09', 'unknown', 'unlock.v.01', 'wait.v.01', 'wake_up.v.02', 'wear.v.02', 'write.v.07'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=84, names=['answer.v.01', 'ask.v.01', 'ask.v.02', 'blow.v.01', 'brandish.v.01', 'break.v.05', 'burn.v.01', 'buy.v.01', 'charge.v.17', 'choose.v.01', 'clean.v.01', 'climb.v.01', 'close.v.01', 'connect.v.01', 'consult.v.02', 'cut.v.01', 'dig.v.01', 'drink.v.01', 'drive.v.01', 'drop.v.01', 'eat.v.01', 'enter.v.01', 'examine.v.02', 'exit.v.01', 'fill.v.01', 'follow.v.01', 'give.v.03', 'hit.v.02', 'hit.v.03', 'insert.v.01', 'insert.v.02', 'inventory.v.01', 'jump.v.01', 'kill.v.01', 'lie_down.v.01', 'light_up.v.05', 'listen.v.01', 'look.v.01', 'lower.v.01', 'memorize.v.01', 'move.v.02', 'note.v.04', 'open.v.01', 'play.v.03', 'pour.v.01', 'pray.v.01', 'press.v.01', 'pull.v.04', 'push.v.01', 'put.v.01', 'raise.v.02', 'read.v.01', 'remove.v.01', 'repeat.v.01', 'rub.v.01', 'say.v.08', 'search.v.04', 'sequence.n.02', 'set.v.05', 'shake.v.01', 'shoot.v.01', 'show.v.01', 'sit_down.v.01', 'skid.v.04', 'sleep.v.01', 'smash.v.02', 'smell.v.01', 'stand.v.03', 'switch_off.v.01', 'switch_on.v.01', 'take.v.04', 'take_off.v.06', 'talk.v.02', 'tell.v.03', 'throw.v.01', 'touch.v.01', 'travel.v.01', 'turn.v.09', 'unknown', 'unlock.v.01', 'wait.v.01', 'wake_up.v.02', 'wear.v.02', 'write.v.07'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=84, names=['answer.v.01', 'ask.v.01', 'ask.v.02', 'blow.v.01', 'brandish.v.01', 'break.v.05', 'burn.v.01', 'buy.v.01', 'charge.v.17', 'choose.v.01', 'clean.v.01', 'climb.v.01', 'close.v.01', 'connect.v.01', 'consult.v.02', 'cut.v.01', 'dig.v.01', 'drink.v.01', 'drive.v.01', 'drop.v.01', 'eat.v.01', 'enter.v.01', 'examine.v.02', 'exit.v.01', 'fill.v.01', 'follow.v.01', 'give.v.03', 'hit.v.02', 'hit.v.03', 'insert.v.01', 'insert.v.02', 'inventory.v.01', 'jump.v.01', 'kill.v.01', 'lie_down.v.01', 'light_up.v.05', 'listen.v.01', 'look.v.01', 'lower.v.01', 'memorize.v.01', 'move.v.02', 'note.v.04', 'open.v.01', 'play.v.03', 'pour.v.01', 'pray.v.01', 'press.v.01', 'pull.v.04', 'push.v.01', 'put.v.01', 'raise.v.02', 'read.v.01', 'remove.v.01', 'repeat.v.01', 'rub.v.01', 'say.v.08', 'search.v.04', 'sequence.n.02', 'set.v.05', 'shake.v.01', 'shoot.v.01', 'show.v.01', 'sit_down.v.01', 'skid.v.04', 'sleep.v.01', 'smash.v.02', 'smell.v.01', 'stand.v.03', 'switch_off.v.01', 'switch_on.v.01', 'take.v.04', 'take_off.v.06', 'talk.v.02', 'tell.v.03', 'throw.v.01', 'touch.v.01', 'travel.v.01', 'turn.v.09', 'unknown', 'unlock.v.01', 'wait.v.01', 'wake_up.v.02', 'wear.v.02', 'write.v.07'], id=None), 'sentence': Value(dtype='string', id=None)}\n",
      "{'idx': 1, 'label': 70, 'sentence': 'take 7'}\n"
     ]
    }
   ],
   "source": [
    "fn_dataset = load_dataset(\"csv\",\n",
    "                          data_files={'train': 'fn_train.tsv',\n",
    "                                      'test': 'fn_test.tsv',\n",
    "                                      'full': 'fn_full.tsv'},\n",
    "                          skiprows=1,\n",
    "                          column_names=['idx', 'label', 'sentence'],\n",
    "                          delimiter=\"\\t\")\n",
    "fn_dataset = fn_dataset.class_encode_column('label')\n",
    "print(fn_dataset)\n",
    "print(fn_dataset['train'].features)\n",
    "print(fn_dataset['test'].features)\n",
    "print(fn_dataset['train'][1])\n",
    "\n",
    "npc_dataset = load_dataset(\"csv\",\n",
    "                           data_files={'train': 'npc_train.tsv',\n",
    "                                       'test': 'npc_test.tsv',\n",
    "                                       'full': 'npc_full.tsv'},\n",
    "                           skiprows=1,\n",
    "                           column_names=['idx', 'label', 'sentence'],\n",
    "                           delimiter=\"\\t\")\n",
    "npc_dataset = npc_dataset.class_encode_column('label')\n",
    "print(npc_dataset)\n",
    "print(npc_dataset['train'].features)\n",
    "print(npc_dataset['test'].features)\n",
    "print(npc_dataset['train'][1])\n",
    "\n",
    "vn_dataset = load_dataset(\"csv\",\n",
    "                          data_files={'train': 'vn_train.tsv',\n",
    "                                      'test': 'vn_test.tsv',\n",
    "                                      'full': 'vn_full.tsv'},\n",
    "                          skiprows=1,\n",
    "                          column_names=['idx', 'label', 'sentence'],\n",
    "                          delimiter=\"\\t\")\n",
    "vn_dataset = vn_dataset.class_encode_column('label')\n",
    "print(vn_dataset)\n",
    "print(vn_dataset['train'].features)\n",
    "print(vn_dataset['test'].features)\n",
    "print(vn_dataset['train'][1])\n",
    "\n",
    "wn_dataset = load_dataset(\"csv\",\n",
    "                          data_files={'train': 'wn_train.tsv',\n",
    "                                      'test': 'wn_test.tsv',\n",
    "                                      'full': 'wn_full.tsv'},\n",
    "                          skiprows=1,\n",
    "                          column_names=['idx', 'label', 'sentence'],\n",
    "                          delimiter=\"\\t\")\n",
    "wn_dataset = wn_dataset.class_encode_column('label')\n",
    "print(wn_dataset)\n",
    "print(wn_dataset['train'].features)\n",
    "print(wn_dataset['test'].features)\n",
    "print(wn_dataset['full'].features)\n",
    "print(wn_dataset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'112': 0, '1132': 1, '1210': 2, '1262': 3, '139': 4, '1390': 5, '16': 6, '160': 7, '1688': 8, '17': 9, '171': 10, '197': 11, '2135': 12, '2161': 13, '254': 14, '263': 15, '264': 16, '265': 17, '273': 18, '276': 19, '279': 20, '2824': 21, '283': 22, '289': 23, '293': 24, '301': 25, '31': 26, '352': 27, '395': 28, '40': 29, '41': 30, '410': 31, '414': 32, '416': 33, '42': 34, '424': 35, '43': 36, '54': 37, '55': 38, '56': 39, '59': 40, '590': 41, '6': 42, '62': 43, '63': 44, '64': 45, '65': 46, '652': 47, '66': 48, '683': 49, '7': 50, '770': 51, '801': 52, 'unknown': 53}\n",
      "{0: '112', 1: '1132', 2: '1210', 3: '1262', 4: '139', 5: '1390', 6: '16', 7: '160', 8: '1688', 9: '17', 10: '171', 11: '197', 12: '2135', 13: '2161', 14: '254', 15: '263', 16: '264', 17: '265', 18: '273', 19: '276', 20: '279', 21: '2824', 22: '283', 23: '289', 24: '293', 25: '301', 26: '31', 27: '352', 28: '395', 29: '40', 30: '41', 31: '410', 32: '414', 33: '416', 34: '42', 35: '424', 36: '43', 37: '54', 38: '55', 39: '56', 40: '59', 41: '590', 42: '6', 43: '62', 44: '63', 45: '64', 46: '65', 47: '652', 48: '66', 49: '683', 50: '7', 51: '770', 52: '801', 53: 'unknown'}\n",
      "{'0': 0, '1': 1, 'unknown': 2}\n",
      "{0: '0', 1: '1', 2: 'unknown'}\n",
      "{'appear-48.1.1': 0, 'assuming_position-50': 1, 'banish-10.2': 2, 'body_internal_states-40.6': 3, 'break-45.1': 4, 'bump-18.4': 5, 'carry-11.4': 6, 'carry-11.4-1-1': 7, 'carve-21.2-2': 8, 'chase-51.6': 9, 'crane-40.3.2': 10, 'cut-21.1-1': 11, 'disassemble-23.3': 12, 'eat-39.1-1': 13, 'eat-39.1-2': 14, 'escape-51.1-1': 15, 'exist-47.1-1': 16, 'feeding-39.7': 17, 'fill-9.8': 18, 'fulfilling-13.4.1': 19, 'funnel-9.3-2-1': 20, 'get-13.5.1': 21, 'give-13.1-1': 22, 'hit-18.1-1': 23, 'hurt-40.8.3-2': 24, 'investigate-35.4': 25, 'knead-26.5': 26, 'learn-14-1': 27, 'learn-14-2-1': 28, 'long-32.2-1': 29, 'manner_speaking-37.3': 30, 'modes_of_being_with_motion-47.3': 31, 'murder-42.1-1': 32, 'other_cos-45.4': 33, 'peer-30.3': 34, 'performance-26.7-1-1': 35, 'pour-9.5': 36, 'preparing-26.3-2': 37, 'push-12-1': 38, 'push-12-1-1': 39, 'put-9.1-2': 40, 'put_direction-9.4': 41, 'put_spatial-9.2-1': 42, 'remove-10.1': 43, 'roll-51.3.1': 44, 'run-51.3.2': 45, 'say-37.7-1': 46, 'scribble-25.2': 47, 'search-35.2': 48, 'see-30.1': 49, 'see-30.1-1': 50, 'shake-22.3-2': 51, 'sight-30.2': 52, 'simple_dressing-41.3.1': 53, 'slide-11.2': 54, 'slide-11.2-1-1': 55, 'snooze-40.4': 56, 'spatial_configuration-47.6': 57, 'split-23.2': 58, 'spray-9.7-1': 59, 'stalk-35.3': 60, 'substance_emission-43.4': 61, 'talk-37.5': 62, 'tape-22.4': 63, 'throw-17.1-1-1': 64, 'touch-20-1': 65, 'transcribe-25.4': 66, 'transfer_mesg-37.1.1': 67, 'unknown': 68, 'wipe_manner-10.4.1-1': 69}\n",
      "{0: 'appear-48.1.1', 1: 'assuming_position-50', 2: 'banish-10.2', 3: 'body_internal_states-40.6', 4: 'break-45.1', 5: 'bump-18.4', 6: 'carry-11.4', 7: 'carry-11.4-1-1', 8: 'carve-21.2-2', 9: 'chase-51.6', 10: 'crane-40.3.2', 11: 'cut-21.1-1', 12: 'disassemble-23.3', 13: 'eat-39.1-1', 14: 'eat-39.1-2', 15: 'escape-51.1-1', 16: 'exist-47.1-1', 17: 'feeding-39.7', 18: 'fill-9.8', 19: 'fulfilling-13.4.1', 20: 'funnel-9.3-2-1', 21: 'get-13.5.1', 22: 'give-13.1-1', 23: 'hit-18.1-1', 24: 'hurt-40.8.3-2', 25: 'investigate-35.4', 26: 'knead-26.5', 27: 'learn-14-1', 28: 'learn-14-2-1', 29: 'long-32.2-1', 30: 'manner_speaking-37.3', 31: 'modes_of_being_with_motion-47.3', 32: 'murder-42.1-1', 33: 'other_cos-45.4', 34: 'peer-30.3', 35: 'performance-26.7-1-1', 36: 'pour-9.5', 37: 'preparing-26.3-2', 38: 'push-12-1', 39: 'push-12-1-1', 40: 'put-9.1-2', 41: 'put_direction-9.4', 42: 'put_spatial-9.2-1', 43: 'remove-10.1', 44: 'roll-51.3.1', 45: 'run-51.3.2', 46: 'say-37.7-1', 47: 'scribble-25.2', 48: 'search-35.2', 49: 'see-30.1', 50: 'see-30.1-1', 51: 'shake-22.3-2', 52: 'sight-30.2', 53: 'simple_dressing-41.3.1', 54: 'slide-11.2', 55: 'slide-11.2-1-1', 56: 'snooze-40.4', 57: 'spatial_configuration-47.6', 58: 'split-23.2', 59: 'spray-9.7-1', 60: 'stalk-35.3', 61: 'substance_emission-43.4', 62: 'talk-37.5', 63: 'tape-22.4', 64: 'throw-17.1-1-1', 65: 'touch-20-1', 66: 'transcribe-25.4', 67: 'transfer_mesg-37.1.1', 68: 'unknown', 69: 'wipe_manner-10.4.1-1'}\n",
      "{'answer.v.01': 0, 'ask.v.01': 1, 'ask.v.02': 2, 'blow.v.01': 3, 'brandish.v.01': 4, 'break.v.05': 5, 'burn.v.01': 6, 'buy.v.01': 7, 'charge.v.17': 8, 'choose.v.01': 9, 'clean.v.01': 10, 'climb.v.01': 11, 'close.v.01': 12, 'connect.v.01': 13, 'consult.v.02': 14, 'cut.v.01': 15, 'dig.v.01': 16, 'drink.v.01': 17, 'drive.v.01': 18, 'drop.v.01': 19, 'eat.v.01': 20, 'enter.v.01': 21, 'examine.v.02': 22, 'exit.v.01': 23, 'fill.v.01': 24, 'follow.v.01': 25, 'give.v.03': 26, 'hit.v.02': 27, 'hit.v.03': 28, 'insert.v.01': 29, 'insert.v.02': 30, 'inventory.v.01': 31, 'jump.v.01': 32, 'kill.v.01': 33, 'lie_down.v.01': 34, 'light_up.v.05': 35, 'listen.v.01': 36, 'look.v.01': 37, 'lower.v.01': 38, 'memorize.v.01': 39, 'move.v.02': 40, 'note.v.04': 41, 'open.v.01': 42, 'play.v.03': 43, 'pour.v.01': 44, 'pray.v.01': 45, 'press.v.01': 46, 'pull.v.04': 47, 'push.v.01': 48, 'put.v.01': 49, 'raise.v.02': 50, 'read.v.01': 51, 'remove.v.01': 52, 'repeat.v.01': 53, 'rub.v.01': 54, 'say.v.08': 55, 'search.v.04': 56, 'sequence.n.02': 57, 'set.v.05': 58, 'shake.v.01': 59, 'shoot.v.01': 60, 'show.v.01': 61, 'sit_down.v.01': 62, 'skid.v.04': 63, 'sleep.v.01': 64, 'smash.v.02': 65, 'smell.v.01': 66, 'stand.v.03': 67, 'switch_off.v.01': 68, 'switch_on.v.01': 69, 'take.v.04': 70, 'take_off.v.06': 71, 'talk.v.02': 72, 'tell.v.03': 73, 'throw.v.01': 74, 'touch.v.01': 75, 'travel.v.01': 76, 'turn.v.09': 77, 'unknown': 78, 'unlock.v.01': 79, 'wait.v.01': 80, 'wake_up.v.02': 81, 'wear.v.02': 82, 'write.v.07': 83}\n",
      "{0: 'answer.v.01', 1: 'ask.v.01', 2: 'ask.v.02', 3: 'blow.v.01', 4: 'brandish.v.01', 5: 'break.v.05', 6: 'burn.v.01', 7: 'buy.v.01', 8: 'charge.v.17', 9: 'choose.v.01', 10: 'clean.v.01', 11: 'climb.v.01', 12: 'close.v.01', 13: 'connect.v.01', 14: 'consult.v.02', 15: 'cut.v.01', 16: 'dig.v.01', 17: 'drink.v.01', 18: 'drive.v.01', 19: 'drop.v.01', 20: 'eat.v.01', 21: 'enter.v.01', 22: 'examine.v.02', 23: 'exit.v.01', 24: 'fill.v.01', 25: 'follow.v.01', 26: 'give.v.03', 27: 'hit.v.02', 28: 'hit.v.03', 29: 'insert.v.01', 30: 'insert.v.02', 31: 'inventory.v.01', 32: 'jump.v.01', 33: 'kill.v.01', 34: 'lie_down.v.01', 35: 'light_up.v.05', 36: 'listen.v.01', 37: 'look.v.01', 38: 'lower.v.01', 39: 'memorize.v.01', 40: 'move.v.02', 41: 'note.v.04', 42: 'open.v.01', 43: 'play.v.03', 44: 'pour.v.01', 45: 'pray.v.01', 46: 'press.v.01', 47: 'pull.v.04', 48: 'push.v.01', 49: 'put.v.01', 50: 'raise.v.02', 51: 'read.v.01', 52: 'remove.v.01', 53: 'repeat.v.01', 54: 'rub.v.01', 55: 'say.v.08', 56: 'search.v.04', 57: 'sequence.n.02', 58: 'set.v.05', 59: 'shake.v.01', 60: 'shoot.v.01', 61: 'show.v.01', 62: 'sit_down.v.01', 63: 'skid.v.04', 64: 'sleep.v.01', 65: 'smash.v.02', 66: 'smell.v.01', 67: 'stand.v.03', 68: 'switch_off.v.01', 69: 'switch_on.v.01', 70: 'take.v.04', 71: 'take_off.v.06', 72: 'talk.v.02', 73: 'tell.v.03', 74: 'throw.v.01', 75: 'touch.v.01', 76: 'travel.v.01', 77: 'turn.v.09', 78: 'unknown', 79: 'unlock.v.01', 80: 'wait.v.01', 81: 'wake_up.v.02', 82: 'wear.v.02', 83: 'write.v.07'}\n"
     ]
    }
   ],
   "source": [
    "fn_label2id = {}\n",
    "fn_id2label = {}\n",
    "for i, l in enumerate(fn_dataset['train'].features['label'].names):\n",
    "    fn_label2id[l] = i\n",
    "    fn_id2label[i] = l\n",
    "print(fn_label2id)\n",
    "print(fn_id2label)\n",
    "\n",
    "npc_label2id = {}\n",
    "npc_id2label = {}\n",
    "for i, l in enumerate(npc_dataset['train'].features['label'].names):\n",
    "    npc_label2id[l] = i\n",
    "    npc_id2label[i] = l\n",
    "print(npc_label2id)\n",
    "print(npc_id2label)\n",
    "\n",
    "vn_label2id = {}\n",
    "vn_id2label = {}\n",
    "for i, l in enumerate(vn_dataset['train'].features['label'].names):\n",
    "    vn_label2id[l] = i\n",
    "    vn_id2label[i] = l\n",
    "print(vn_label2id)\n",
    "print(vn_id2label)\n",
    "\n",
    "wn_label2id = {}\n",
    "wn_id2label = {}\n",
    "for i, l in enumerate(wn_dataset['train'].features['label'].names):\n",
    "    wn_label2id[l] = i\n",
    "    wn_id2label[i] = l\n",
    "print(wn_label2id)\n",
    "print(wn_id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tOtxt8nwbhWT"
   },
   "outputs": [],
   "source": [
    "# GLUE: https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb\n",
    "GLUE_TASKS_1 = [  # AutoModelForSequenceClassification\n",
    "    (\"cola\", 2, None),\n",
    "]\n",
    "\n",
    "# QA: https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n",
    "SQUAD_TASKS = [ # AutoModelForQuestionAnswering\n",
    "    (\"squad_v2\", None, \"cola\")\n",
    "]\n",
    "\n",
    "GLUE_TASKS_2 = [  # AutoModelForSequenceClassification\n",
    "    (\"sst2\", 2, \"squad_v2\"),\n",
    "    (\"qqp\", 2, \"sst2\"),\n",
    "    (\"rte\", 2, \"qqp\"),\n",
    "]\n",
    "\n",
    "# Token Classification: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "CHUCK_TASKS = [ # AutoModelForTokenClassification\n",
    "    (\"chunk\", len(label_list), \"rte\")\n",
    "]\n",
    "\n",
    "GLUE_TASKS_3 = [  # AutoModelForSequenceClassification\n",
    "    (\"mrpc\", 2, \"chunk\"),\n",
    "    (\"stsb\", 1, \"mrpc\"),\n",
    "    (\"wnli\", 2, \"stsb\"),\n",
    "    (\"mnli\", 3, \"wnli\"),\n",
    "]\n",
    "\n",
    "JERICHO_TASKS = [  # AutoModelForSequenceClassification\n",
    "    (\"npc\", 3, \"mnli\"),\n",
    "    (\"fn\", 54, \"npc\"),\n",
    "    (\"vn\", 70, \"fn\"),\n",
    "    (\"wn\", 84, \"vn\")\n",
    "]\n",
    "\n",
    "JERICHO_TASKS_FULL = [  # AutoModelForSequenceClassification\n",
    "    (\"npc_full\", 3, \"mnli\"),\n",
    "    (\"fn_full\", 54, \"npc_full\"),\n",
    "    (\"vn_full\", 70, \"fn_full\"),\n",
    "    (\"wn_full\", 84, \"vn_full\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "40b835a49f214e0daf4ac7ae54b6cb11",
      "a1b1ede4f6974900a6bf4c7d86b14750",
      "c012505f502c4a8a945fb687d7df3a09",
      "12c744fa32af4c459414acaf2f55e9f7",
      "a54a947a944c4fe4bb0fd9270085aa96",
      "d5e1493257224bf799d0f73c3cb469b5",
      "6a49eff8c3a14666b80d38bc2ec3cd09",
      "141e43316cb7494190cc49f240a79862",
      "c971a9fc375d408e9883522e910a174c",
      "84886deb6941402b91c91bc04d958bab",
      "2e16f4c2b7a746f6a434ec955ea32642",
      "3c43bec5cd074c19ba8e6aa23d73ddbc",
      "100f5feca8674f529435216ddbd4e822",
      "eeab2c2b2ce1498984b2d101fc21981c",
      "3b6472c99b7a4f20b1bcaa3269982011",
      "2e503116470c4b13825304cd7fe0b422",
      "a8294f4700464990ade651c2aa4b26ad",
      "eed87323b88247998fbbfa177324da90",
      "898440a4b85140a98afdf358288b9906",
      "9a0557e54db44c9798635ac6bd5b9c76",
      "424ca82295f44fd19e7b87f7cb82dd81",
      "21e974b84b7e4fc09e78f1ab9894966f",
      "43365b4b2ccc4fd08262f7c30e4a82b7",
      "4c69c3394f7846aaa88942863c2e9533",
      "5aaaca4b50574a7f9ad716836c7af2d7",
      "eabe7a29e1a445e2924e04e695fa4a2e",
      "030c493bd2f24cbeac5aff32c84e08ab",
      "f9883302291d4455b692ece7e671f375",
      "45df48cc8cad4614a4d3a7fcaa6166fb",
      "51e17e3b39524516950c7473ed5a353f",
      "b5d7e554e0284c0d9b6bc5001314cfd8",
      "9f2aedc08ed4496f84e112337fd456ad",
      "8436922250a54afe87986720c0744cee",
      "bd6a5290ba484bcca0ec681780bb8ae1",
      "7ee3137f83eb495aa4c11f919007bd58",
      "3cda73a4ed074b898941cf52849551de",
      "1c05f629e4b347dcb75a361fdd4827e4",
      "a87a8bda98034f12bacd371f684adf3f",
      "4bb34f0b1cdd41a4bf4d4519704de094",
      "b0e090bc046c4457a9c08b11a5fb7e64",
      "40b35b96bd9543d38b32cc058f4fc993",
      "45f1ba0298cb48c5b576966fe35f76f3",
      "49546cb3476f4b1a8ad5b08e73f4e18c",
      "938fe1a7361840fabdcbb224650f4d42",
      "08dfc845c7744d1b92bf1a5f37b505a7",
      "665d89b7c97542c1bc4b60fdb8379064",
      "1d6f43056f4141eda307c272ce35f67d",
      "ec219ab860d347c1b40550486a9a7d39",
      "167ad294664f4ebda553dcaaf5bb5b10",
      "7f873ce8f5f847c6876f3aba0114fd14",
      "1ad0f0bffaa8457a8f89ee3f591a671e",
      "dfda8acabf3547aba8dc04e56d04c24a",
      "24f3285bc54c477096767941796a4a43",
      "422ab5565f284667bad486a4e4ffd9e0",
      "8c4ebef655e44aebb551406d925d65f1",
      "1d03571ce8324a0da4a0149469b6259c",
      "e14b98564eff4edc81563035d7ec60ec",
      "b26b90f6b8a54a8b9ef933f76fdc8b35",
      "8ffe3e0def1040bd8154831e7fb34896",
      "c14fa0725cc14f358d176598b48a0109",
      "e8419492d48749b992f4618555590ab8",
      "4ea509409ae24c83ad484774746ea1da",
      "be812d748b0d436aa4835bda48501d8c",
      "48c34e464e6e488b972f8445dcf175cf",
      "f8e2f2823bc04aa792255f0b9bff4be7",
      "0e5f745e6b9d4a688206ddadcf23642d",
      "5701d76e624549d5a4c7009444b36676",
      "72e6bac9a1aa4be6a6449a7561555821",
      "75f1ead51877488589d4357feaee2c9c",
      "abde2728613741bcac502f6e573fc78f",
      "1b620978aea943f59eeb76b43fef49a1",
      "97551b6e0b974831a82131fb00c7b385",
      "7046b07581974b0cb9e860d55b5bdc7a",
      "94b015044199410ca35b2478fba4fc7f",
      "20ade413d1a24164a7c5938cc196a156",
      "a5ed13c17ec14b799dda4531449a236c",
      "f8b0f35d26fc456085547393bdbb0f18"
     ]
    },
    "id": "_fTQ2utFl520",
    "outputId": "48546c8b-352a-4ac6-aaa0-0c667282929e"
   },
   "outputs": [],
   "source": [
    "#for task, num_labels, prev in GLUE_TASKS_1 + SQUAD_TASKS + GLUE_TASKS_2 + CHUCK_TASKS + GLUE_TASKS_3 + JERICHO_TASKS:\n",
    "def run(task, num_labels, prev):\n",
    "    if task == \"squad_v2\":\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"chunk\":\n",
    "        model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"fn\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=fn_label2id, id2label=fn_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"npc\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=npc_label2id, id2label=npc_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"vn\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=vn_label2id, id2label=vn_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"wn\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=wn_label2id, id2label=wn_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "\n",
    "    # print(model)\n",
    "    if task == \"squad_v2\":\n",
    "        raw_datasets = load_dataset(\"squad_v2\")\n",
    "    elif task == \"chunk\":\n",
    "        raw_datasets = load_dataset(\"conll2003\")\n",
    "    elif task == \"fn\":\n",
    "        raw_datasets = fn_dataset\n",
    "    elif task == \"npc\":\n",
    "        raw_datasets = npc_dataset\n",
    "    elif task == \"vn\":\n",
    "        raw_datasets = vn_dataset\n",
    "    elif task == \"wn\":\n",
    "        raw_datasets = wn_dataset\n",
    "    else:\n",
    "        raw_datasets = load_dataset(\"glue\", task)\n",
    "\n",
    "\n",
    "    # print(raw_datasets)\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        if task == \"cola\" or task == \"sst2\":\n",
    "            return tokenizer(example[\"sentence\"], truncation=True, stride=128)\n",
    "        if task == \"qqp\":\n",
    "            return tokenizer(example[\"question1\"], example[\"question2\"], truncation=True, stride=128)\n",
    "        if task == \"rte\" or task == \"mrpc\" or task == \"stsb\" or task == \"wnli\":\n",
    "            return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, stride=128)\n",
    "        if task == \"mnli\":\n",
    "            return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, stride=128)\n",
    "        if task == \"chunk\":\n",
    "            tokenized_inputs = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, stride=128)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(example[\"chunk_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    else:\n",
    "                        label_ids.append(label[word_idx])\n",
    "\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "        if task == \"squad_v2\":\n",
    "            tokenized_examples = tokenizer(\n",
    "                example[\"question\" if tokenizer.padding_side == \"right\" else \"context\"],\n",
    "                example[\"context\" if tokenizer.padding_side == \"right\" else \"question\"],\n",
    "                truncation=\"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\",\n",
    "                stride=128,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "            offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "            tokenized_examples[\"start_positions\"] = []\n",
    "            tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "            for i, offsets in enumerate(offset_mapping):\n",
    "                input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "                cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "                sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "                sample_index = sample_mapping[i]\n",
    "                answers = example[\"answers\"][sample_index]\n",
    "                if len(answers[\"answer_start\"]) == 0:\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    start_char = answers[\"answer_start\"][0]\n",
    "                    end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                    token_start_index = 0\n",
    "                    while sequence_ids[token_start_index] != (1 if tokenizer.padding_side == \"right\" else 0):\n",
    "                        token_start_index += 1\n",
    "\n",
    "                    token_end_index = len(input_ids) - 1\n",
    "                    while sequence_ids[token_end_index] != (1 if tokenizer.padding_side == \"right\" else 0):\n",
    "                        token_end_index -= 1\n",
    "\n",
    "                    if not (\n",
    "                            offsets[token_start_index][0] <= start_char\n",
    "                            and offsets[token_end_index][1] >= end_char\n",
    "                    ):\n",
    "                        tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                        tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                    else:\n",
    "                        while (\n",
    "                                token_start_index < len(offsets)\n",
    "                                and offsets[token_start_index][0] <= start_char\n",
    "                        ):\n",
    "                            token_start_index += 1\n",
    "                        tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                        while offsets[token_end_index][1] >= end_char:\n",
    "                            token_end_index -= 1\n",
    "                        tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "            return tokenized_examples\n",
    "        return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "\n",
    "    if task == \"chunk\":\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    else:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets[\n",
    "        \"train\"].column_names if task == \"squad_v2\" else None)\n",
    "    \n",
    "    # print(tokenized_datasets['train'][3])\n",
    "    \n",
    "    training_args = TrainingArguments(f\"{task}-trainer\",\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      optim=\"adamw_torch\",\n",
    "                                      learning_rate=1e-4,\n",
    "                                      weight_decay=0.01,\n",
    "                                      warmup_ratio=0.1,\n",
    "                                      adam_epsilon=1e-6,\n",
    "                                      num_train_epochs=10.0 if task == \"npc\" or task == \"vn\" or task == \"wn\" or task == \"fn\" else 7.0,\n",
    "                                      save_strategy=\"epoch\",\n",
    "                                      evaluation_strategy=\"epoch\",\n",
    "                                      \n",
    "                                      # Debug\n",
    "                                      #save_steps=2,\n",
    "                                      #eval_steps =2,\n",
    "                                      #save_strategy=\"steps\",\n",
    "                                      #evaluation_strategy=\"steps\",\n",
    "                                      #max_steps=4,\n",
    "                                      \n",
    "                                      save_total_limit=1,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      per_device_train_batch_size=32,\n",
    "                                      per_device_eval_batch_size=32)\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        if task == \"squad_v2\":\n",
    "            metric = load_metric(\"squad_v2\")\n",
    "        elif task == \"chunk\":\n",
    "            metric = load_metric(\"seqeval\")\n",
    "        elif task == \"npc\" or task == \"vn\" or task == \"wn\" or task == \"fn\":\n",
    "            metric = load_metric(\"accuracy\")\n",
    "        else:\n",
    "            metric = load_metric(\"glue\", task)\n",
    "\n",
    "        logits, labels = eval_preds\n",
    "        if task == \"stsb\":\n",
    "            predictions = logits[:, 0]\n",
    "        elif task == \"chunk\":\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "        else:\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        if task == \"chunk\":\n",
    "            true_predictions = [\n",
    "                [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            true_labels = [\n",
    "                [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "            return {\n",
    "                \"precision\": results[\"overall_precision\"],\n",
    "                \"recall\": results[\"overall_recall\"],\n",
    "                \"f1\": results[\"overall_f1\"],\n",
    "                \"accuracy\": results[\"overall_accuracy\"],\n",
    "            }\n",
    "        else:\n",
    "            return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation_matched\" if task == \"mnli\" else (\"test\" if task == \"npc\" or task == \"vn\" or task == \"wn\" or task == \"fn\" else \"validation\")],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=None if task == \"squad_v2\" else compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.model.electra.save_pretrained(f\"{task}-trainer\")\n",
    "    if prev is not None and prev != \"mnli\" and os.path.exists(f\"{prev}-trainer\"):\n",
    "        try:\n",
    "            shutil.rmtree(f\"{prev}-trainer\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0377cd9807340bd830fcf5a6dbcbe33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-55d00ae1d67fa65d.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6dd882ef945e0019.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4dee9a92b94586b733cacd56a77fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 8551\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1876\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1876' max='1876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1876/1876 03:15, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.542559</td>\n",
       "      <td>0.439084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.507689</td>\n",
       "      <td>0.496658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.504116</td>\n",
       "      <td>0.553419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.534901</td>\n",
       "      <td>0.545334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.665376</td>\n",
       "      <td>0.563576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.733317</td>\n",
       "      <td>0.573205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.747631</td>\n",
       "      <td>0.574092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-268\n",
      "Configuration saved in cola-trainer/checkpoint-268/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-268/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-268/special_tokens_map.json\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-536] due to args.save_total_limit\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-1876] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-536\n",
      "Configuration saved in cola-trainer/checkpoint-536/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-536/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-536/special_tokens_map.json\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-268] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-804\n",
      "Configuration saved in cola-trainer/checkpoint-804/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-804/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-804/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-804/special_tokens_map.json\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-536] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-1072\n",
      "Configuration saved in cola-trainer/checkpoint-1072/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-1072/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-1072/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-1072/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-1340\n",
      "Configuration saved in cola-trainer/checkpoint-1340/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-1340/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-1340/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-1340/special_tokens_map.json\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-1072] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-1608\n",
      "Configuration saved in cola-trainer/checkpoint-1608/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-1608/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-1608/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-1608/special_tokens_map.json\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-1340] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to cola-trainer/checkpoint-1876\n",
      "Configuration saved in cola-trainer/checkpoint-1876/config.json\n",
      "Model weights saved in cola-trainer/checkpoint-1876/pytorch_model.bin\n",
      "tokenizer config file saved in cola-trainer/checkpoint-1876/tokenizer_config.json\n",
      "Special tokens file saved in cola-trainer/checkpoint-1876/special_tokens_map.json\n",
      "Deleting older checkpoint [cola-trainer/checkpoint-1608] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from cola-trainer/checkpoint-804 (score: 0.5041157603263855).\n",
      "Configuration saved in cola-trainer/config.json\n",
      "Model weights saved in cola-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"cola\", 2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file cola-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"cola-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file cola-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at cola-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset squad_v2 (/home/ap_default/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80543a7836c94f77b3370ed48c2ff20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-cacda3fb52f40d9a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a142869e05b4c5fabd3cd374b84c503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 130503\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28553' max='28553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28553/28553 10:29:04, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.343600</td>\n",
       "      <td>1.358325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.072100</td>\n",
       "      <td>1.194226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>1.136428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.708500</td>\n",
       "      <td>1.176706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>1.319421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>1.549892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>1.620008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-4079\n",
      "Configuration saved in squad_v2-trainer/checkpoint-4079/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-4079/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-4079/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-4079/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-8158\n",
      "Configuration saved in squad_v2-trainer/checkpoint-8158/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-8158/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-8158/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-8158/special_tokens_map.json\n",
      "Deleting older checkpoint [squad_v2-trainer/checkpoint-4079] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-12237\n",
      "Configuration saved in squad_v2-trainer/checkpoint-12237/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-12237/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-12237/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-12237/special_tokens_map.json\n",
      "Deleting older checkpoint [squad_v2-trainer/checkpoint-8158] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-16316\n",
      "Configuration saved in squad_v2-trainer/checkpoint-16316/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-16316/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-16316/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-16316/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-20395\n",
      "Configuration saved in squad_v2-trainer/checkpoint-20395/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-20395/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-20395/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-20395/special_tokens_map.json\n",
      "Deleting older checkpoint [squad_v2-trainer/checkpoint-16316] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-24474\n",
      "Configuration saved in squad_v2-trainer/checkpoint-24474/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-24474/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-24474/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-24474/special_tokens_map.json\n",
      "Deleting older checkpoint [squad_v2-trainer/checkpoint-20395] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11969\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to squad_v2-trainer/checkpoint-28553\n",
      "Configuration saved in squad_v2-trainer/checkpoint-28553/config.json\n",
      "Model weights saved in squad_v2-trainer/checkpoint-28553/pytorch_model.bin\n",
      "tokenizer config file saved in squad_v2-trainer/checkpoint-28553/tokenizer_config.json\n",
      "Special tokens file saved in squad_v2-trainer/checkpoint-28553/special_tokens_map.json\n",
      "Deleting older checkpoint [squad_v2-trainer/checkpoint-24474] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from squad_v2-trainer/checkpoint-12237 (score: 1.1364275217056274).\n",
      "Configuration saved in squad_v2-trainer/config.json\n",
      "Model weights saved in squad_v2-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"squad_v2\", None, \"cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file squad_v2-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"squad_v2-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file squad_v2-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at squad_v2-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220800146a4c450b8a5585761af968c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f1f6aeea9742b8b534898ebba2dabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25053f5ae4d3417a840e14e44c5ed6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11a9037ae894dd0a819b8d46b00de24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14735\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14735' max='14735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14735/14735 30:35, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.360923</td>\n",
       "      <td>0.857798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.353908</td>\n",
       "      <td>0.891055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>0.891055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.386930</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.373171</td>\n",
       "      <td>0.897936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.428789</td>\n",
       "      <td>0.895642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.473749</td>\n",
       "      <td>0.901376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-2105\n",
      "Configuration saved in sst2-trainer/checkpoint-2105/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-2105/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-2105/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-2105/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-4210\n",
      "Configuration saved in sst2-trainer/checkpoint-4210/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-4210/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-4210/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-4210/special_tokens_map.json\n",
      "Deleting older checkpoint [sst2-trainer/checkpoint-2105] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-6315\n",
      "Configuration saved in sst2-trainer/checkpoint-6315/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-6315/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-6315/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-6315/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-8420\n",
      "Configuration saved in sst2-trainer/checkpoint-8420/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-8420/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-8420/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-8420/special_tokens_map.json\n",
      "Deleting older checkpoint [sst2-trainer/checkpoint-6315] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-10525\n",
      "Configuration saved in sst2-trainer/checkpoint-10525/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-10525/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-10525/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-10525/special_tokens_map.json\n",
      "Deleting older checkpoint [sst2-trainer/checkpoint-8420] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-12630\n",
      "Configuration saved in sst2-trainer/checkpoint-12630/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-12630/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-12630/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-12630/special_tokens_map.json\n",
      "Deleting older checkpoint [sst2-trainer/checkpoint-10525] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to sst2-trainer/checkpoint-14735\n",
      "Configuration saved in sst2-trainer/checkpoint-14735/config.json\n",
      "Model weights saved in sst2-trainer/checkpoint-14735/pytorch_model.bin\n",
      "tokenizer config file saved in sst2-trainer/checkpoint-14735/tokenizer_config.json\n",
      "Special tokens file saved in sst2-trainer/checkpoint-14735/special_tokens_map.json\n",
      "Deleting older checkpoint [sst2-trainer/checkpoint-12630] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from sst2-trainer/checkpoint-4210 (score: 0.35390788316726685).\n",
      "Configuration saved in sst2-trainer/config.json\n",
      "Model weights saved in sst2-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"sst2\", 2, \"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file sst2-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"sst2-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file sst2-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at sst2-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5bb37bfe88476590b9e162c5b048d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d443cb8e78e499683d1ce4e61c8dda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/364 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c53ee2be49417da5ca1c99a81346e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e1174ab29543a6b7235192ea0880be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 363846\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 79597\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79597' max='79597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79597/79597 4:00:50, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.292421</td>\n",
       "      <td>0.871655</td>\n",
       "      <td>0.829941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.277175</td>\n",
       "      <td>0.881103</td>\n",
       "      <td>0.843231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.265185</td>\n",
       "      <td>0.888053</td>\n",
       "      <td>0.857530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.266624</td>\n",
       "      <td>0.898269</td>\n",
       "      <td>0.863360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.277629</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.870597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.336603</td>\n",
       "      <td>0.901731</td>\n",
       "      <td>0.871461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.394128</td>\n",
       "      <td>0.904081</td>\n",
       "      <td>0.871436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-11371\n",
      "Configuration saved in qqp-trainer/checkpoint-11371/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-11371/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-11371/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-11371/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-22742\n",
      "Configuration saved in qqp-trainer/checkpoint-22742/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-22742/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-22742/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-22742/special_tokens_map.json\n",
      "Deleting older checkpoint [qqp-trainer/checkpoint-11371] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-34113\n",
      "Configuration saved in qqp-trainer/checkpoint-34113/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-34113/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-34113/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-34113/special_tokens_map.json\n",
      "Deleting older checkpoint [qqp-trainer/checkpoint-22742] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-45484\n",
      "Configuration saved in qqp-trainer/checkpoint-45484/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-45484/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-45484/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-45484/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-56855\n",
      "Configuration saved in qqp-trainer/checkpoint-56855/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-56855/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-56855/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-56855/special_tokens_map.json\n",
      "Deleting older checkpoint [qqp-trainer/checkpoint-45484] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-68226\n",
      "Configuration saved in qqp-trainer/checkpoint-68226/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-68226/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-68226/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-68226/special_tokens_map.json\n",
      "Deleting older checkpoint [qqp-trainer/checkpoint-56855] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40430\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to qqp-trainer/checkpoint-79597\n",
      "Configuration saved in qqp-trainer/checkpoint-79597/config.json\n",
      "Model weights saved in qqp-trainer/checkpoint-79597/pytorch_model.bin\n",
      "tokenizer config file saved in qqp-trainer/checkpoint-79597/tokenizer_config.json\n",
      "Special tokens file saved in qqp-trainer/checkpoint-79597/special_tokens_map.json\n",
      "Deleting older checkpoint [qqp-trainer/checkpoint-68226] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from qqp-trainer/checkpoint-34113 (score: 0.26518476009368896).\n",
      "Configuration saved in qqp-trainer/config.json\n",
      "Model weights saved in qqp-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"qqp\", 2, \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file qqp-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"qqp-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file qqp-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at qqp-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980b9f02083d490faa4f361ac6938eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1f3e73f6864a618ea5cc7c6f90bdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9be3ed089504839bae16f62273c6860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc8103828134e8db1dc87b13876a935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2490\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 03:56, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692772</td>\n",
       "      <td>0.509025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668118</td>\n",
       "      <td>0.592058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644537</td>\n",
       "      <td>0.631769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698660</td>\n",
       "      <td>0.642599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744542</td>\n",
       "      <td>0.642599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.769668</td>\n",
       "      <td>0.646209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.814780</td>\n",
       "      <td>0.631769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-78\n",
      "Configuration saved in rte-trainer/checkpoint-78/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-78/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-78/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-78/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-156\n",
      "Configuration saved in rte-trainer/checkpoint-156/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-156/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-156/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-156/special_tokens_map.json\n",
      "Deleting older checkpoint [rte-trainer/checkpoint-78] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-234\n",
      "Configuration saved in rte-trainer/checkpoint-234/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-234/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-234/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-234/special_tokens_map.json\n",
      "Deleting older checkpoint [rte-trainer/checkpoint-156] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-312\n",
      "Configuration saved in rte-trainer/checkpoint-312/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-312/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-312/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-312/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-390\n",
      "Configuration saved in rte-trainer/checkpoint-390/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-390/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-390/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-390/special_tokens_map.json\n",
      "Deleting older checkpoint [rte-trainer/checkpoint-312] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-468\n",
      "Configuration saved in rte-trainer/checkpoint-468/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-468/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-468/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-468/special_tokens_map.json\n",
      "Deleting older checkpoint [rte-trainer/checkpoint-390] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to rte-trainer/checkpoint-546\n",
      "Configuration saved in rte-trainer/checkpoint-546/config.json\n",
      "Model weights saved in rte-trainer/checkpoint-546/pytorch_model.bin\n",
      "tokenizer config file saved in rte-trainer/checkpoint-546/tokenizer_config.json\n",
      "Special tokens file saved in rte-trainer/checkpoint-546/special_tokens_map.json\n",
      "Deleting older checkpoint [rte-trainer/checkpoint-468] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from rte-trainer/checkpoint-234 (score: 0.6445373296737671).\n",
      "Configuration saved in rte-trainer/config.json\n",
      "Model weights saved in rte-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"rte\", 2, \"qqp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file rte-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"rte-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file rte-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at rte-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset conll2003 (/home/ap_default/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347a6024f30f407aaa4fa11053caeccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28b9b9823114183a9656f63bd4574af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf729dc960204fa99f484e0edc9616c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560fef9d81f84e6bbddb6c80233822b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14042\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3073\n",
      "Trainer is attempting to log a value of \"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3073' max='3073' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3073/3073 08:12, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319847</td>\n",
       "      <td>0.845958</td>\n",
       "      <td>0.858499</td>\n",
       "      <td>0.852183</td>\n",
       "      <td>0.918249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.026400</td>\n",
       "      <td>0.237982</td>\n",
       "      <td>0.885404</td>\n",
       "      <td>0.888150</td>\n",
       "      <td>0.886775</td>\n",
       "      <td>0.938345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.222445</td>\n",
       "      <td>0.905267</td>\n",
       "      <td>0.895385</td>\n",
       "      <td>0.900299</td>\n",
       "      <td>0.945256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.208001</td>\n",
       "      <td>0.909959</td>\n",
       "      <td>0.898937</td>\n",
       "      <td>0.904415</td>\n",
       "      <td>0.947591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.212159</td>\n",
       "      <td>0.911461</td>\n",
       "      <td>0.902393</td>\n",
       "      <td>0.906904</td>\n",
       "      <td>0.948576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>0.907739</td>\n",
       "      <td>0.904719</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>0.948735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.211576</td>\n",
       "      <td>0.911387</td>\n",
       "      <td>0.904557</td>\n",
       "      <td>0.907959</td>\n",
       "      <td>0.949450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-439\n",
      "Configuration saved in chunk-trainer/checkpoint-439/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-439/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-439/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-439/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-878\n",
      "Configuration saved in chunk-trainer/checkpoint-878/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-878/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-878/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-878/special_tokens_map.json\n",
      "Deleting older checkpoint [chunk-trainer/checkpoint-439] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-1317\n",
      "Configuration saved in chunk-trainer/checkpoint-1317/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-1317/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-1317/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-1317/special_tokens_map.json\n",
      "Deleting older checkpoint [chunk-trainer/checkpoint-878] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-1756\n",
      "Configuration saved in chunk-trainer/checkpoint-1756/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-1756/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-1756/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-1756/special_tokens_map.json\n",
      "Deleting older checkpoint [chunk-trainer/checkpoint-1317] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-2195\n",
      "Configuration saved in chunk-trainer/checkpoint-2195/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-2195/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-2195/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-2195/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-2634\n",
      "Configuration saved in chunk-trainer/checkpoint-2634/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-2634/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-2634/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-2634/special_tokens_map.json\n",
      "Deleting older checkpoint [chunk-trainer/checkpoint-1756] due to args.save_total_limit\n",
      "Deleting older checkpoint [chunk-trainer/checkpoint-2195] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: pos_tags, id, tokens, chunk_tags, ner_tags. If pos_tags, id, tokens, chunk_tags, ner_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 32\n",
      "/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to chunk-trainer/checkpoint-3073\n",
      "Configuration saved in chunk-trainer/checkpoint-3073/config.json\n",
      "Model weights saved in chunk-trainer/checkpoint-3073/pytorch_model.bin\n",
      "tokenizer config file saved in chunk-trainer/checkpoint-3073/tokenizer_config.json\n",
      "Special tokens file saved in chunk-trainer/checkpoint-3073/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from chunk-trainer/checkpoint-2634 (score: 0.2040088176727295).\n",
      "Configuration saved in chunk-trainer/config.json\n",
      "Model weights saved in chunk-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"chunk\", len(label_list), \"rte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file chunk-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"chunk-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file chunk-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at chunk-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d702d7331b4067bf19594f63aa0b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895d2c5f82284b2584db6896e4633c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8b293ef38240ae920e405470d809ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ee760086c04ac89b966a04beb76a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 805\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='805' max='805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [805/805 02:49, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.700980</td>\n",
       "      <td>0.820059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.525511</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.834921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500907</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.827119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586037</td>\n",
       "      <td>0.776961</td>\n",
       "      <td>0.849587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.635375</td>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.842282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.749043</td>\n",
       "      <td>0.781863</td>\n",
       "      <td>0.845217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.778755</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.848168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-115\n",
      "Configuration saved in mrpc-trainer/checkpoint-115/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-115/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-115/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-115/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-230\n",
      "Configuration saved in mrpc-trainer/checkpoint-230/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-230/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-230/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-230/special_tokens_map.json\n",
      "Deleting older checkpoint [mrpc-trainer/checkpoint-115] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-345\n",
      "Configuration saved in mrpc-trainer/checkpoint-345/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [mrpc-trainer/checkpoint-230] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-460\n",
      "Configuration saved in mrpc-trainer/checkpoint-460/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-460/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-460/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-460/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-575\n",
      "Configuration saved in mrpc-trainer/checkpoint-575/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-575/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-575/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-575/special_tokens_map.json\n",
      "Deleting older checkpoint [mrpc-trainer/checkpoint-460] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-690\n",
      "Configuration saved in mrpc-trainer/checkpoint-690/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [mrpc-trainer/checkpoint-575] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mrpc-trainer/checkpoint-805\n",
      "Configuration saved in mrpc-trainer/checkpoint-805/config.json\n",
      "Model weights saved in mrpc-trainer/checkpoint-805/pytorch_model.bin\n",
      "tokenizer config file saved in mrpc-trainer/checkpoint-805/tokenizer_config.json\n",
      "Special tokens file saved in mrpc-trainer/checkpoint-805/special_tokens_map.json\n",
      "Deleting older checkpoint [mrpc-trainer/checkpoint-690] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from mrpc-trainer/checkpoint-345 (score: 0.5009072422981262).\n",
      "Configuration saved in mrpc-trainer/config.json\n",
      "Model weights saved in mrpc-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"mrpc\", 2, \"chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file mrpc-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"mrpc-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file mrpc-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at mrpc-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b1298cc8b143169bfec23073100a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcade029aa34a618ae7fa3d5e8b0143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768a6640a6cd4494be5843e43ca649f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635160d4deec4f1fa26a5161e45e9cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5749\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1260' max='1260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1260/1260 04:02, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652621</td>\n",
       "      <td>0.845683</td>\n",
       "      <td>0.841065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.606566</td>\n",
       "      <td>0.866593</td>\n",
       "      <td>0.864842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.722800</td>\n",
       "      <td>0.599615</td>\n",
       "      <td>0.871527</td>\n",
       "      <td>0.869917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.722800</td>\n",
       "      <td>0.594279</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.872540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.722800</td>\n",
       "      <td>0.591404</td>\n",
       "      <td>0.872534</td>\n",
       "      <td>0.871335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.239500</td>\n",
       "      <td>0.579900</td>\n",
       "      <td>0.874185</td>\n",
       "      <td>0.871654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.239500</td>\n",
       "      <td>0.592205</td>\n",
       "      <td>0.874243</td>\n",
       "      <td>0.871459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-180\n",
      "Configuration saved in stsb-trainer/checkpoint-180/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-180/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-180/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-180/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-360\n",
      "Configuration saved in stsb-trainer/checkpoint-360/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-360/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-360/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-360/special_tokens_map.json\n",
      "Deleting older checkpoint [stsb-trainer/checkpoint-180] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-540\n",
      "Configuration saved in stsb-trainer/checkpoint-540/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-540/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-540/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-540/special_tokens_map.json\n",
      "Deleting older checkpoint [stsb-trainer/checkpoint-360] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-720\n",
      "Configuration saved in stsb-trainer/checkpoint-720/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-720/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-720/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-720/special_tokens_map.json\n",
      "Deleting older checkpoint [stsb-trainer/checkpoint-540] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-900\n",
      "Configuration saved in stsb-trainer/checkpoint-900/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [stsb-trainer/checkpoint-720] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-1080\n",
      "Configuration saved in stsb-trainer/checkpoint-1080/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-1080/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-1080/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-1080/special_tokens_map.json\n",
      "Deleting older checkpoint [stsb-trainer/checkpoint-900] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to stsb-trainer/checkpoint-1260\n",
      "Configuration saved in stsb-trainer/checkpoint-1260/config.json\n",
      "Model weights saved in stsb-trainer/checkpoint-1260/pytorch_model.bin\n",
      "tokenizer config file saved in stsb-trainer/checkpoint-1260/tokenizer_config.json\n",
      "Special tokens file saved in stsb-trainer/checkpoint-1260/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from stsb-trainer/checkpoint-1080 (score: 0.5798995494842529).\n",
      "Configuration saved in stsb-trainer/config.json\n",
      "Model weights saved in stsb-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"stsb\", 1, \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file stsb-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"stsb-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file stsb-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at stsb-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad50bc62d4a4314878525a09ba1eae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947a871985054dc5a9eb06cc32709c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c99fc6fcd846f1b1d37f92d8bcbe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896619730fc74e6cab0a61d243dac53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 635\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 00:34, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688438</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692742</td>\n",
       "      <td>0.464789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693777</td>\n",
       "      <td>0.450704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691511</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694226</td>\n",
       "      <td>0.450704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696077</td>\n",
       "      <td>0.366197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696820</td>\n",
       "      <td>0.394366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-20\n",
      "Configuration saved in wnli-trainer/checkpoint-20/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-20/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-20/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-40\n",
      "Configuration saved in wnli-trainer/checkpoint-40/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-40/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-60\n",
      "Configuration saved in wnli-trainer/checkpoint-60/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-60/special_tokens_map.json\n",
      "Deleting older checkpoint [wnli-trainer/checkpoint-40] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-80\n",
      "Configuration saved in wnli-trainer/checkpoint-80/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [wnli-trainer/checkpoint-60] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-100\n",
      "Configuration saved in wnli-trainer/checkpoint-100/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [wnli-trainer/checkpoint-80] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-120\n",
      "Configuration saved in wnli-trainer/checkpoint-120/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-120/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-120/special_tokens_map.json\n",
      "Deleting older checkpoint [wnli-trainer/checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wnli-trainer/checkpoint-140\n",
      "Configuration saved in wnli-trainer/checkpoint-140/config.json\n",
      "Model weights saved in wnli-trainer/checkpoint-140/pytorch_model.bin\n",
      "tokenizer config file saved in wnli-trainer/checkpoint-140/tokenizer_config.json\n",
      "Special tokens file saved in wnli-trainer/checkpoint-140/special_tokens_map.json\n",
      "Deleting older checkpoint [wnli-trainer/checkpoint-120] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from wnli-trainer/checkpoint-20 (score: 0.6884379982948303).\n",
      "Configuration saved in wnli-trainer/config.json\n",
      "Model weights saved in wnli-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"wnli\", 2, \"stsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file wnli-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"wnli-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file wnli-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at wnli-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Reusing dataset glue (/home/ap_default/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139a40bb616743f982bb84b6f251e393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584dc708713410f81c01873e8afd29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2731f84fa541b1a2b707973b2efa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fd8d990a52419dbf9756bae7c5efd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bcee3b15aa4a1d8890bc02afe11919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fc35f97b194d999b98387876a3e91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 392702\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 85904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85904' max='85904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [85904/85904 5:23:50, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>0.601395</td>\n",
       "      <td>0.763016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.570949</td>\n",
       "      <td>0.783902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.476400</td>\n",
       "      <td>0.529574</td>\n",
       "      <td>0.798879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.538816</td>\n",
       "      <td>0.802955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.596635</td>\n",
       "      <td>0.804381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.279100</td>\n",
       "      <td>0.635879</td>\n",
       "      <td>0.804279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.689345</td>\n",
       "      <td>0.807030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-12272\n",
      "Configuration saved in mnli-trainer/checkpoint-12272/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-12272/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-12272/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-12272/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-24544\n",
      "Configuration saved in mnli-trainer/checkpoint-24544/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-24544/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-24544/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-24544/special_tokens_map.json\n",
      "Deleting older checkpoint [mnli-trainer/checkpoint-12272] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-36816\n",
      "Configuration saved in mnli-trainer/checkpoint-36816/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-36816/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-36816/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-36816/special_tokens_map.json\n",
      "Deleting older checkpoint [mnli-trainer/checkpoint-24544] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-49088\n",
      "Configuration saved in mnli-trainer/checkpoint-49088/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-49088/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-49088/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-49088/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-61360\n",
      "Configuration saved in mnli-trainer/checkpoint-61360/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-61360/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-61360/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-61360/special_tokens_map.json\n",
      "Deleting older checkpoint [mnli-trainer/checkpoint-49088] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-73632\n",
      "Configuration saved in mnli-trainer/checkpoint-73632/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-73632/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-73632/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-73632/special_tokens_map.json\n",
      "Deleting older checkpoint [mnli-trainer/checkpoint-61360] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mnli-trainer/checkpoint-85904\n",
      "Configuration saved in mnli-trainer/checkpoint-85904/config.json\n",
      "Model weights saved in mnli-trainer/checkpoint-85904/pytorch_model.bin\n",
      "tokenizer config file saved in mnli-trainer/checkpoint-85904/tokenizer_config.json\n",
      "Special tokens file saved in mnli-trainer/checkpoint-85904/special_tokens_map.json\n",
      "Deleting older checkpoint [mnli-trainer/checkpoint-73632] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from mnli-trainer/checkpoint-36816 (score: 0.5295737385749817).\n",
      "Configuration saved in mnli-trainer/config.json\n",
      "Model weights saved in mnli-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"mnli\", 3, \"wnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"unknown\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"unknown\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file mnli-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"mnli-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file mnli-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at mnli-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87bdb6d7d1c4303ba31bce0fd9d5371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fab538f131479fa85fba90b7dccef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dff126678e47f1aa1451b1ba25b6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3327\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 01:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.066416</td>\n",
       "      <td>0.989183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.993990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.029964</td>\n",
       "      <td>0.993990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.022454</td>\n",
       "      <td>0.997596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.029048</td>\n",
       "      <td>0.996394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.020444</td>\n",
       "      <td>0.996394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.008506</td>\n",
       "      <td>0.997596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.998798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.998798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-104\n",
      "Configuration saved in npc-trainer/checkpoint-104/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-104/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-104/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-208\n",
      "Configuration saved in npc-trainer/checkpoint-208/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-208/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-104] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-312\n",
      "Configuration saved in npc-trainer/checkpoint-312/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-312/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-312/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-312/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-208] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-416\n",
      "Configuration saved in npc-trainer/checkpoint-416/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-416/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-416/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-416/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-312] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-520\n",
      "Configuration saved in npc-trainer/checkpoint-520/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-520/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-520/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-520/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-624\n",
      "Configuration saved in npc-trainer/checkpoint-624/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-624/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-624/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-624/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-416] due to args.save_total_limit\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-520] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-728\n",
      "Configuration saved in npc-trainer/checkpoint-728/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-728/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-728/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-728/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-624] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-832\n",
      "Configuration saved in npc-trainer/checkpoint-832/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-832/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-832/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-832/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-728] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-936\n",
      "Configuration saved in npc-trainer/checkpoint-936/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-936/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-936/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-936/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-832] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 832\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to npc-trainer/checkpoint-1040\n",
      "Configuration saved in npc-trainer/checkpoint-1040/config.json\n",
      "Model weights saved in npc-trainer/checkpoint-1040/pytorch_model.bin\n",
      "tokenizer config file saved in npc-trainer/checkpoint-1040/tokenizer_config.json\n",
      "Special tokens file saved in npc-trainer/checkpoint-1040/special_tokens_map.json\n",
      "Deleting older checkpoint [npc-trainer/checkpoint-936] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from npc-trainer/checkpoint-1040 (score: 0.0017587407492101192).\n",
      "Configuration saved in npc-trainer/config.json\n",
      "Model weights saved in npc-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"npc\", 3, \"mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"112\",\n",
      "    \"1\": \"1132\",\n",
      "    \"2\": \"1210\",\n",
      "    \"3\": \"1262\",\n",
      "    \"4\": \"139\",\n",
      "    \"5\": \"1390\",\n",
      "    \"6\": \"16\",\n",
      "    \"7\": \"160\",\n",
      "    \"8\": \"1688\",\n",
      "    \"9\": \"17\",\n",
      "    \"10\": \"171\",\n",
      "    \"11\": \"197\",\n",
      "    \"12\": \"2135\",\n",
      "    \"13\": \"2161\",\n",
      "    \"14\": \"254\",\n",
      "    \"15\": \"263\",\n",
      "    \"16\": \"264\",\n",
      "    \"17\": \"265\",\n",
      "    \"18\": \"273\",\n",
      "    \"19\": \"276\",\n",
      "    \"20\": \"279\",\n",
      "    \"21\": \"2824\",\n",
      "    \"22\": \"283\",\n",
      "    \"23\": \"289\",\n",
      "    \"24\": \"293\",\n",
      "    \"25\": \"301\",\n",
      "    \"26\": \"31\",\n",
      "    \"27\": \"352\",\n",
      "    \"28\": \"395\",\n",
      "    \"29\": \"40\",\n",
      "    \"30\": \"41\",\n",
      "    \"31\": \"410\",\n",
      "    \"32\": \"414\",\n",
      "    \"33\": \"416\",\n",
      "    \"34\": \"42\",\n",
      "    \"35\": \"424\",\n",
      "    \"36\": \"43\",\n",
      "    \"37\": \"54\",\n",
      "    \"38\": \"55\",\n",
      "    \"39\": \"56\",\n",
      "    \"40\": \"59\",\n",
      "    \"41\": \"590\",\n",
      "    \"42\": \"6\",\n",
      "    \"43\": \"62\",\n",
      "    \"44\": \"63\",\n",
      "    \"45\": \"64\",\n",
      "    \"46\": \"65\",\n",
      "    \"47\": \"652\",\n",
      "    \"48\": \"66\",\n",
      "    \"49\": \"683\",\n",
      "    \"50\": \"7\",\n",
      "    \"51\": \"770\",\n",
      "    \"52\": \"801\",\n",
      "    \"53\": \"unknown\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"112\": 0,\n",
      "    \"1132\": 1,\n",
      "    \"1210\": 2,\n",
      "    \"1262\": 3,\n",
      "    \"139\": 4,\n",
      "    \"1390\": 5,\n",
      "    \"16\": 6,\n",
      "    \"160\": 7,\n",
      "    \"1688\": 8,\n",
      "    \"17\": 9,\n",
      "    \"171\": 10,\n",
      "    \"197\": 11,\n",
      "    \"2135\": 12,\n",
      "    \"2161\": 13,\n",
      "    \"254\": 14,\n",
      "    \"263\": 15,\n",
      "    \"264\": 16,\n",
      "    \"265\": 17,\n",
      "    \"273\": 18,\n",
      "    \"276\": 19,\n",
      "    \"279\": 20,\n",
      "    \"2824\": 21,\n",
      "    \"283\": 22,\n",
      "    \"289\": 23,\n",
      "    \"293\": 24,\n",
      "    \"301\": 25,\n",
      "    \"31\": 26,\n",
      "    \"352\": 27,\n",
      "    \"395\": 28,\n",
      "    \"40\": 29,\n",
      "    \"41\": 30,\n",
      "    \"410\": 31,\n",
      "    \"414\": 32,\n",
      "    \"416\": 33,\n",
      "    \"42\": 34,\n",
      "    \"424\": 35,\n",
      "    \"43\": 36,\n",
      "    \"54\": 37,\n",
      "    \"55\": 38,\n",
      "    \"56\": 39,\n",
      "    \"59\": 40,\n",
      "    \"590\": 41,\n",
      "    \"6\": 42,\n",
      "    \"62\": 43,\n",
      "    \"63\": 44,\n",
      "    \"64\": 45,\n",
      "    \"65\": 46,\n",
      "    \"652\": 47,\n",
      "    \"66\": 48,\n",
      "    \"683\": 49,\n",
      "    \"7\": 50,\n",
      "    \"770\": 51,\n",
      "    \"801\": 52,\n",
      "    \"unknown\": 53\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file npc-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"npc-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file npc-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at npc-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97fceecf8af4489a903254dca006a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e7a6fa74ff4aa48a9c8d21f80eb7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400bc35face943a38bf2e09d43e12622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3329\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1050\n",
      "Trainer is attempting to log a value of \"{0: '112', 1: '1132', 2: '1210', 3: '1262', 4: '139', 5: '1390', 6: '16', 7: '160', 8: '1688', 9: '17', 10: '171', 11: '197', 12: '2135', 13: '2161', 14: '254', 15: '263', 16: '264', 17: '265', 18: '273', 19: '276', 20: '279', 21: '2824', 22: '283', 23: '289', 24: '293', 25: '301', 26: '31', 27: '352', 28: '395', 29: '40', 30: '41', 31: '410', 32: '414', 33: '416', 34: '42', 35: '424', 36: '43', 37: '54', 38: '55', 39: '56', 40: '59', 41: '590', 42: '6', 43: '62', 44: '63', 45: '64', 46: '65', 47: '652', 48: '66', 49: '683', 50: '7', 51: '770', 52: '801', 53: 'unknown'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'112': 0, '1132': 1, '1210': 2, '1262': 3, '139': 4, '1390': 5, '16': 6, '160': 7, '1688': 8, '17': 9, '171': 10, '197': 11, '2135': 12, '2161': 13, '254': 14, '263': 15, '264': 16, '265': 17, '273': 18, '276': 19, '279': 20, '2824': 21, '283': 22, '289': 23, '293': 24, '301': 25, '31': 26, '352': 27, '395': 28, '40': 29, '41': 30, '410': 31, '414': 32, '416': 33, '42': 34, '424': 35, '43': 36, '54': 37, '55': 38, '56': 39, '59': 40, '590': 41, '6': 42, '62': 43, '63': 44, '64': 45, '65': 46, '652': 47, '66': 48, '683': 49, '7': 50, '770': 51, '801': 52, 'unknown': 53}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 01:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.001414</td>\n",
       "      <td>0.335366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.732983</td>\n",
       "      <td>0.570732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.169175</td>\n",
       "      <td>0.757317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912826</td>\n",
       "      <td>0.789024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.957400</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>0.809756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.957400</td>\n",
       "      <td>0.748937</td>\n",
       "      <td>0.817073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.957400</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>0.813415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.957400</td>\n",
       "      <td>0.660422</td>\n",
       "      <td>0.834146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.957400</td>\n",
       "      <td>0.642226</td>\n",
       "      <td>0.832927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.654800</td>\n",
       "      <td>0.638219</td>\n",
       "      <td>0.835366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-105\n",
      "Configuration saved in fn-trainer/checkpoint-105/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-105/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-105/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-105/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-210\n",
      "Configuration saved in fn-trainer/checkpoint-210/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-210/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-210/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-210/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-105] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-315\n",
      "Configuration saved in fn-trainer/checkpoint-315/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-315/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-315/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-315/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-210] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-420\n",
      "Configuration saved in fn-trainer/checkpoint-420/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-420/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-420/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-420/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-315] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-525\n",
      "Configuration saved in fn-trainer/checkpoint-525/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-525/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-525/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-525/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-420] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-630\n",
      "Configuration saved in fn-trainer/checkpoint-630/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-630/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-630/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-630/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-525] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-735\n",
      "Configuration saved in fn-trainer/checkpoint-735/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-735/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-735/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-735/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-630] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-840\n",
      "Configuration saved in fn-trainer/checkpoint-840/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-840/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-840/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-840/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-735] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-945\n",
      "Configuration saved in fn-trainer/checkpoint-945/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-945/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-945/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-945/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-840] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 820\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to fn-trainer/checkpoint-1050\n",
      "Configuration saved in fn-trainer/checkpoint-1050/config.json\n",
      "Model weights saved in fn-trainer/checkpoint-1050/pytorch_model.bin\n",
      "tokenizer config file saved in fn-trainer/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in fn-trainer/checkpoint-1050/special_tokens_map.json\n",
      "Deleting older checkpoint [fn-trainer/checkpoint-945] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from fn-trainer/checkpoint-1050 (score: 0.6382188200950623).\n",
      "Configuration saved in fn-trainer/config.json\n",
      "Model weights saved in fn-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"fn\", 54, \"npc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"appear-48.1.1\",\n",
      "    \"1\": \"assuming_position-50\",\n",
      "    \"2\": \"banish-10.2\",\n",
      "    \"3\": \"body_internal_states-40.6\",\n",
      "    \"4\": \"break-45.1\",\n",
      "    \"5\": \"bump-18.4\",\n",
      "    \"6\": \"carry-11.4\",\n",
      "    \"7\": \"carry-11.4-1-1\",\n",
      "    \"8\": \"carve-21.2-2\",\n",
      "    \"9\": \"chase-51.6\",\n",
      "    \"10\": \"crane-40.3.2\",\n",
      "    \"11\": \"cut-21.1-1\",\n",
      "    \"12\": \"disassemble-23.3\",\n",
      "    \"13\": \"eat-39.1-1\",\n",
      "    \"14\": \"eat-39.1-2\",\n",
      "    \"15\": \"escape-51.1-1\",\n",
      "    \"16\": \"exist-47.1-1\",\n",
      "    \"17\": \"feeding-39.7\",\n",
      "    \"18\": \"fill-9.8\",\n",
      "    \"19\": \"fulfilling-13.4.1\",\n",
      "    \"20\": \"funnel-9.3-2-1\",\n",
      "    \"21\": \"get-13.5.1\",\n",
      "    \"22\": \"give-13.1-1\",\n",
      "    \"23\": \"hit-18.1-1\",\n",
      "    \"24\": \"hurt-40.8.3-2\",\n",
      "    \"25\": \"investigate-35.4\",\n",
      "    \"26\": \"knead-26.5\",\n",
      "    \"27\": \"learn-14-1\",\n",
      "    \"28\": \"learn-14-2-1\",\n",
      "    \"29\": \"long-32.2-1\",\n",
      "    \"30\": \"manner_speaking-37.3\",\n",
      "    \"31\": \"modes_of_being_with_motion-47.3\",\n",
      "    \"32\": \"murder-42.1-1\",\n",
      "    \"33\": \"other_cos-45.4\",\n",
      "    \"34\": \"peer-30.3\",\n",
      "    \"35\": \"performance-26.7-1-1\",\n",
      "    \"36\": \"pour-9.5\",\n",
      "    \"37\": \"preparing-26.3-2\",\n",
      "    \"38\": \"push-12-1\",\n",
      "    \"39\": \"push-12-1-1\",\n",
      "    \"40\": \"put-9.1-2\",\n",
      "    \"41\": \"put_direction-9.4\",\n",
      "    \"42\": \"put_spatial-9.2-1\",\n",
      "    \"43\": \"remove-10.1\",\n",
      "    \"44\": \"roll-51.3.1\",\n",
      "    \"45\": \"run-51.3.2\",\n",
      "    \"46\": \"say-37.7-1\",\n",
      "    \"47\": \"scribble-25.2\",\n",
      "    \"48\": \"search-35.2\",\n",
      "    \"49\": \"see-30.1\",\n",
      "    \"50\": \"see-30.1-1\",\n",
      "    \"51\": \"shake-22.3-2\",\n",
      "    \"52\": \"sight-30.2\",\n",
      "    \"53\": \"simple_dressing-41.3.1\",\n",
      "    \"54\": \"slide-11.2\",\n",
      "    \"55\": \"slide-11.2-1-1\",\n",
      "    \"56\": \"snooze-40.4\",\n",
      "    \"57\": \"spatial_configuration-47.6\",\n",
      "    \"58\": \"split-23.2\",\n",
      "    \"59\": \"spray-9.7-1\",\n",
      "    \"60\": \"stalk-35.3\",\n",
      "    \"61\": \"substance_emission-43.4\",\n",
      "    \"62\": \"talk-37.5\",\n",
      "    \"63\": \"tape-22.4\",\n",
      "    \"64\": \"throw-17.1-1-1\",\n",
      "    \"65\": \"touch-20-1\",\n",
      "    \"66\": \"transcribe-25.4\",\n",
      "    \"67\": \"transfer_mesg-37.1.1\",\n",
      "    \"68\": \"unknown\",\n",
      "    \"69\": \"wipe_manner-10.4.1-1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"appear-48.1.1\": 0,\n",
      "    \"assuming_position-50\": 1,\n",
      "    \"banish-10.2\": 2,\n",
      "    \"body_internal_states-40.6\": 3,\n",
      "    \"break-45.1\": 4,\n",
      "    \"bump-18.4\": 5,\n",
      "    \"carry-11.4\": 6,\n",
      "    \"carry-11.4-1-1\": 7,\n",
      "    \"carve-21.2-2\": 8,\n",
      "    \"chase-51.6\": 9,\n",
      "    \"crane-40.3.2\": 10,\n",
      "    \"cut-21.1-1\": 11,\n",
      "    \"disassemble-23.3\": 12,\n",
      "    \"eat-39.1-1\": 13,\n",
      "    \"eat-39.1-2\": 14,\n",
      "    \"escape-51.1-1\": 15,\n",
      "    \"exist-47.1-1\": 16,\n",
      "    \"feeding-39.7\": 17,\n",
      "    \"fill-9.8\": 18,\n",
      "    \"fulfilling-13.4.1\": 19,\n",
      "    \"funnel-9.3-2-1\": 20,\n",
      "    \"get-13.5.1\": 21,\n",
      "    \"give-13.1-1\": 22,\n",
      "    \"hit-18.1-1\": 23,\n",
      "    \"hurt-40.8.3-2\": 24,\n",
      "    \"investigate-35.4\": 25,\n",
      "    \"knead-26.5\": 26,\n",
      "    \"learn-14-1\": 27,\n",
      "    \"learn-14-2-1\": 28,\n",
      "    \"long-32.2-1\": 29,\n",
      "    \"manner_speaking-37.3\": 30,\n",
      "    \"modes_of_being_with_motion-47.3\": 31,\n",
      "    \"murder-42.1-1\": 32,\n",
      "    \"other_cos-45.4\": 33,\n",
      "    \"peer-30.3\": 34,\n",
      "    \"performance-26.7-1-1\": 35,\n",
      "    \"pour-9.5\": 36,\n",
      "    \"preparing-26.3-2\": 37,\n",
      "    \"push-12-1\": 38,\n",
      "    \"push-12-1-1\": 39,\n",
      "    \"put-9.1-2\": 40,\n",
      "    \"put_direction-9.4\": 41,\n",
      "    \"put_spatial-9.2-1\": 42,\n",
      "    \"remove-10.1\": 43,\n",
      "    \"roll-51.3.1\": 44,\n",
      "    \"run-51.3.2\": 45,\n",
      "    \"say-37.7-1\": 46,\n",
      "    \"scribble-25.2\": 47,\n",
      "    \"search-35.2\": 48,\n",
      "    \"see-30.1\": 49,\n",
      "    \"see-30.1-1\": 50,\n",
      "    \"shake-22.3-2\": 51,\n",
      "    \"sight-30.2\": 52,\n",
      "    \"simple_dressing-41.3.1\": 53,\n",
      "    \"slide-11.2\": 54,\n",
      "    \"slide-11.2-1-1\": 55,\n",
      "    \"snooze-40.4\": 56,\n",
      "    \"spatial_configuration-47.6\": 57,\n",
      "    \"split-23.2\": 58,\n",
      "    \"spray-9.7-1\": 59,\n",
      "    \"stalk-35.3\": 60,\n",
      "    \"substance_emission-43.4\": 61,\n",
      "    \"talk-37.5\": 62,\n",
      "    \"tape-22.4\": 63,\n",
      "    \"throw-17.1-1-1\": 64,\n",
      "    \"touch-20-1\": 65,\n",
      "    \"transcribe-25.4\": 66,\n",
      "    \"transfer_mesg-37.1.1\": 67,\n",
      "    \"unknown\": 68,\n",
      "    \"wipe_manner-10.4.1-1\": 69\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file fn-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"fn-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file fn-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at fn-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363674554bfa490caacdd7015000c01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a18bda46f541ffbde660bf2aaaa99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f370ea78464b0a99849700da0c0b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4718\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n",
      "Trainer is attempting to log a value of \"{0: 'appear-48.1.1', 1: 'assuming_position-50', 2: 'banish-10.2', 3: 'body_internal_states-40.6', 4: 'break-45.1', 5: 'bump-18.4', 6: 'carry-11.4', 7: 'carry-11.4-1-1', 8: 'carve-21.2-2', 9: 'chase-51.6', 10: 'crane-40.3.2', 11: 'cut-21.1-1', 12: 'disassemble-23.3', 13: 'eat-39.1-1', 14: 'eat-39.1-2', 15: 'escape-51.1-1', 16: 'exist-47.1-1', 17: 'feeding-39.7', 18: 'fill-9.8', 19: 'fulfilling-13.4.1', 20: 'funnel-9.3-2-1', 21: 'get-13.5.1', 22: 'give-13.1-1', 23: 'hit-18.1-1', 24: 'hurt-40.8.3-2', 25: 'investigate-35.4', 26: 'knead-26.5', 27: 'learn-14-1', 28: 'learn-14-2-1', 29: 'long-32.2-1', 30: 'manner_speaking-37.3', 31: 'modes_of_being_with_motion-47.3', 32: 'murder-42.1-1', 33: 'other_cos-45.4', 34: 'peer-30.3', 35: 'performance-26.7-1-1', 36: 'pour-9.5', 37: 'preparing-26.3-2', 38: 'push-12-1', 39: 'push-12-1-1', 40: 'put-9.1-2', 41: 'put_direction-9.4', 42: 'put_spatial-9.2-1', 43: 'remove-10.1', 44: 'roll-51.3.1', 45: 'run-51.3.2', 46: 'say-37.7-1', 47: 'scribble-25.2', 48: 'search-35.2', 49: 'see-30.1', 50: 'see-30.1-1', 51: 'shake-22.3-2', 52: 'sight-30.2', 53: 'simple_dressing-41.3.1', 54: 'slide-11.2', 55: 'slide-11.2-1-1', 56: 'snooze-40.4', 57: 'spatial_configuration-47.6', 58: 'split-23.2', 59: 'spray-9.7-1', 60: 'stalk-35.3', 61: 'substance_emission-43.4', 62: 'talk-37.5', 63: 'tape-22.4', 64: 'throw-17.1-1-1', 65: 'touch-20-1', 66: 'transcribe-25.4', 67: 'transfer_mesg-37.1.1', 68: 'unknown', 69: 'wipe_manner-10.4.1-1'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'appear-48.1.1': 0, 'assuming_position-50': 1, 'banish-10.2': 2, 'body_internal_states-40.6': 3, 'break-45.1': 4, 'bump-18.4': 5, 'carry-11.4': 6, 'carry-11.4-1-1': 7, 'carve-21.2-2': 8, 'chase-51.6': 9, 'crane-40.3.2': 10, 'cut-21.1-1': 11, 'disassemble-23.3': 12, 'eat-39.1-1': 13, 'eat-39.1-2': 14, 'escape-51.1-1': 15, 'exist-47.1-1': 16, 'feeding-39.7': 17, 'fill-9.8': 18, 'fulfilling-13.4.1': 19, 'funnel-9.3-2-1': 20, 'get-13.5.1': 21, 'give-13.1-1': 22, 'hit-18.1-1': 23, 'hurt-40.8.3-2': 24, 'investigate-35.4': 25, 'knead-26.5': 26, 'learn-14-1': 27, 'learn-14-2-1': 28, 'long-32.2-1': 29, 'manner_speaking-37.3': 30, 'modes_of_being_with_motion-47.3': 31, 'murder-42.1-1': 32, 'other_cos-45.4': 33, 'peer-30.3': 34, 'performance-26.7-1-1': 35, 'pour-9.5': 36, 'preparing-26.3-2': 37, 'push-12-1': 38, 'push-12-1-1': 39, 'put-9.1-2': 40, 'put_direction-9.4': 41, 'put_spatial-9.2-1': 42, 'remove-10.1': 43, 'roll-51.3.1': 44, 'run-51.3.2': 45, 'say-37.7-1': 46, 'scribble-25.2': 47, 'search-35.2': 48, 'see-30.1': 49, 'see-30.1-1': 50, 'shake-22.3-2': 51, 'sight-30.2': 52, 'simple_dressing-41.3.1': 53, 'slide-11.2': 54, 'slide-11.2-1-1': 55, 'snooze-40.4': 56, 'spatial_configuration-47.6': 57, 'split-23.2': 58, 'spray-9.7-1': 59, 'stalk-35.3': 60, 'substance_emission-43.4': 61, 'talk-37.5': 62, 'tape-22.4': 63, 'throw-17.1-1-1': 64, 'touch-20-1': 65, 'transcribe-25.4': 66, 'transfer_mesg-37.1.1': 67, 'unknown': 68, 'wipe_manner-10.4.1-1': 69}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 02:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.459750</td>\n",
       "      <td>0.469671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.531336</td>\n",
       "      <td>0.515598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.218851</td>\n",
       "      <td>0.556326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.225600</td>\n",
       "      <td>1.074604</td>\n",
       "      <td>0.583189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.225600</td>\n",
       "      <td>1.004782</td>\n",
       "      <td>0.595321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.225600</td>\n",
       "      <td>0.953358</td>\n",
       "      <td>0.622184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.008100</td>\n",
       "      <td>0.924729</td>\n",
       "      <td>0.618718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.008100</td>\n",
       "      <td>0.898267</td>\n",
       "      <td>0.629983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.008100</td>\n",
       "      <td>0.881558</td>\n",
       "      <td>0.631716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.008100</td>\n",
       "      <td>0.876060</td>\n",
       "      <td>0.625650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-148\n",
      "Configuration saved in vn-trainer/checkpoint-148/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-148/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-148/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-148/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-296\n",
      "Configuration saved in vn-trainer/checkpoint-296/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-296/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-296/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-296/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-148] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-444\n",
      "Configuration saved in vn-trainer/checkpoint-444/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-444/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-444/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-444/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-296] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-592\n",
      "Configuration saved in vn-trainer/checkpoint-592/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-592/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-592/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-592/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-444] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-740\n",
      "Configuration saved in vn-trainer/checkpoint-740/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-740/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-740/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-740/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-592] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-888\n",
      "Configuration saved in vn-trainer/checkpoint-888/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-888/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-888/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-888/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-740] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-1036\n",
      "Configuration saved in vn-trainer/checkpoint-1036/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-1036/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-1036/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-1036/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-888] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-1184\n",
      "Configuration saved in vn-trainer/checkpoint-1184/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-1184/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-1184/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-1184/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-1036] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-1332\n",
      "Configuration saved in vn-trainer/checkpoint-1332/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-1332/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-1332/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-1332/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-1184] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1154\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to vn-trainer/checkpoint-1480\n",
      "Configuration saved in vn-trainer/checkpoint-1480/config.json\n",
      "Model weights saved in vn-trainer/checkpoint-1480/pytorch_model.bin\n",
      "tokenizer config file saved in vn-trainer/checkpoint-1480/tokenizer_config.json\n",
      "Special tokens file saved in vn-trainer/checkpoint-1480/special_tokens_map.json\n",
      "Deleting older checkpoint [vn-trainer/checkpoint-1332] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vn-trainer/checkpoint-1480 (score: 0.8760598301887512).\n",
      "Configuration saved in vn-trainer/config.json\n",
      "Model weights saved in vn-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"vn\", 70, \"fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"answer.v.01\",\n",
      "    \"1\": \"ask.v.01\",\n",
      "    \"2\": \"ask.v.02\",\n",
      "    \"3\": \"blow.v.01\",\n",
      "    \"4\": \"brandish.v.01\",\n",
      "    \"5\": \"break.v.05\",\n",
      "    \"6\": \"burn.v.01\",\n",
      "    \"7\": \"buy.v.01\",\n",
      "    \"8\": \"charge.v.17\",\n",
      "    \"9\": \"choose.v.01\",\n",
      "    \"10\": \"clean.v.01\",\n",
      "    \"11\": \"climb.v.01\",\n",
      "    \"12\": \"close.v.01\",\n",
      "    \"13\": \"connect.v.01\",\n",
      "    \"14\": \"consult.v.02\",\n",
      "    \"15\": \"cut.v.01\",\n",
      "    \"16\": \"dig.v.01\",\n",
      "    \"17\": \"drink.v.01\",\n",
      "    \"18\": \"drive.v.01\",\n",
      "    \"19\": \"drop.v.01\",\n",
      "    \"20\": \"eat.v.01\",\n",
      "    \"21\": \"enter.v.01\",\n",
      "    \"22\": \"examine.v.02\",\n",
      "    \"23\": \"exit.v.01\",\n",
      "    \"24\": \"fill.v.01\",\n",
      "    \"25\": \"follow.v.01\",\n",
      "    \"26\": \"give.v.03\",\n",
      "    \"27\": \"hit.v.02\",\n",
      "    \"28\": \"hit.v.03\",\n",
      "    \"29\": \"insert.v.01\",\n",
      "    \"30\": \"insert.v.02\",\n",
      "    \"31\": \"inventory.v.01\",\n",
      "    \"32\": \"jump.v.01\",\n",
      "    \"33\": \"kill.v.01\",\n",
      "    \"34\": \"lie_down.v.01\",\n",
      "    \"35\": \"light_up.v.05\",\n",
      "    \"36\": \"listen.v.01\",\n",
      "    \"37\": \"look.v.01\",\n",
      "    \"38\": \"lower.v.01\",\n",
      "    \"39\": \"memorize.v.01\",\n",
      "    \"40\": \"move.v.02\",\n",
      "    \"41\": \"note.v.04\",\n",
      "    \"42\": \"open.v.01\",\n",
      "    \"43\": \"play.v.03\",\n",
      "    \"44\": \"pour.v.01\",\n",
      "    \"45\": \"pray.v.01\",\n",
      "    \"46\": \"press.v.01\",\n",
      "    \"47\": \"pull.v.04\",\n",
      "    \"48\": \"push.v.01\",\n",
      "    \"49\": \"put.v.01\",\n",
      "    \"50\": \"raise.v.02\",\n",
      "    \"51\": \"read.v.01\",\n",
      "    \"52\": \"remove.v.01\",\n",
      "    \"53\": \"repeat.v.01\",\n",
      "    \"54\": \"rub.v.01\",\n",
      "    \"55\": \"say.v.08\",\n",
      "    \"56\": \"search.v.04\",\n",
      "    \"57\": \"sequence.n.02\",\n",
      "    \"58\": \"set.v.05\",\n",
      "    \"59\": \"shake.v.01\",\n",
      "    \"60\": \"shoot.v.01\",\n",
      "    \"61\": \"show.v.01\",\n",
      "    \"62\": \"sit_down.v.01\",\n",
      "    \"63\": \"skid.v.04\",\n",
      "    \"64\": \"sleep.v.01\",\n",
      "    \"65\": \"smash.v.02\",\n",
      "    \"66\": \"smell.v.01\",\n",
      "    \"67\": \"stand.v.03\",\n",
      "    \"68\": \"switch_off.v.01\",\n",
      "    \"69\": \"switch_on.v.01\",\n",
      "    \"70\": \"take.v.04\",\n",
      "    \"71\": \"take_off.v.06\",\n",
      "    \"72\": \"talk.v.02\",\n",
      "    \"73\": \"tell.v.03\",\n",
      "    \"74\": \"throw.v.01\",\n",
      "    \"75\": \"touch.v.01\",\n",
      "    \"76\": \"travel.v.01\",\n",
      "    \"77\": \"turn.v.09\",\n",
      "    \"78\": \"unknown\",\n",
      "    \"79\": \"unlock.v.01\",\n",
      "    \"80\": \"wait.v.01\",\n",
      "    \"81\": \"wake_up.v.02\",\n",
      "    \"82\": \"wear.v.02\",\n",
      "    \"83\": \"write.v.07\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"answer.v.01\": 0,\n",
      "    \"ask.v.01\": 1,\n",
      "    \"ask.v.02\": 2,\n",
      "    \"blow.v.01\": 3,\n",
      "    \"brandish.v.01\": 4,\n",
      "    \"break.v.05\": 5,\n",
      "    \"burn.v.01\": 6,\n",
      "    \"buy.v.01\": 7,\n",
      "    \"charge.v.17\": 8,\n",
      "    \"choose.v.01\": 9,\n",
      "    \"clean.v.01\": 10,\n",
      "    \"climb.v.01\": 11,\n",
      "    \"close.v.01\": 12,\n",
      "    \"connect.v.01\": 13,\n",
      "    \"consult.v.02\": 14,\n",
      "    \"cut.v.01\": 15,\n",
      "    \"dig.v.01\": 16,\n",
      "    \"drink.v.01\": 17,\n",
      "    \"drive.v.01\": 18,\n",
      "    \"drop.v.01\": 19,\n",
      "    \"eat.v.01\": 20,\n",
      "    \"enter.v.01\": 21,\n",
      "    \"examine.v.02\": 22,\n",
      "    \"exit.v.01\": 23,\n",
      "    \"fill.v.01\": 24,\n",
      "    \"follow.v.01\": 25,\n",
      "    \"give.v.03\": 26,\n",
      "    \"hit.v.02\": 27,\n",
      "    \"hit.v.03\": 28,\n",
      "    \"insert.v.01\": 29,\n",
      "    \"insert.v.02\": 30,\n",
      "    \"inventory.v.01\": 31,\n",
      "    \"jump.v.01\": 32,\n",
      "    \"kill.v.01\": 33,\n",
      "    \"lie_down.v.01\": 34,\n",
      "    \"light_up.v.05\": 35,\n",
      "    \"listen.v.01\": 36,\n",
      "    \"look.v.01\": 37,\n",
      "    \"lower.v.01\": 38,\n",
      "    \"memorize.v.01\": 39,\n",
      "    \"move.v.02\": 40,\n",
      "    \"note.v.04\": 41,\n",
      "    \"open.v.01\": 42,\n",
      "    \"play.v.03\": 43,\n",
      "    \"pour.v.01\": 44,\n",
      "    \"pray.v.01\": 45,\n",
      "    \"press.v.01\": 46,\n",
      "    \"pull.v.04\": 47,\n",
      "    \"push.v.01\": 48,\n",
      "    \"put.v.01\": 49,\n",
      "    \"raise.v.02\": 50,\n",
      "    \"read.v.01\": 51,\n",
      "    \"remove.v.01\": 52,\n",
      "    \"repeat.v.01\": 53,\n",
      "    \"rub.v.01\": 54,\n",
      "    \"say.v.08\": 55,\n",
      "    \"search.v.04\": 56,\n",
      "    \"sequence.n.02\": 57,\n",
      "    \"set.v.05\": 58,\n",
      "    \"shake.v.01\": 59,\n",
      "    \"shoot.v.01\": 60,\n",
      "    \"show.v.01\": 61,\n",
      "    \"sit_down.v.01\": 62,\n",
      "    \"skid.v.04\": 63,\n",
      "    \"sleep.v.01\": 64,\n",
      "    \"smash.v.02\": 65,\n",
      "    \"smell.v.01\": 66,\n",
      "    \"stand.v.03\": 67,\n",
      "    \"switch_off.v.01\": 68,\n",
      "    \"switch_on.v.01\": 69,\n",
      "    \"take.v.04\": 70,\n",
      "    \"take_off.v.06\": 71,\n",
      "    \"talk.v.02\": 72,\n",
      "    \"tell.v.03\": 73,\n",
      "    \"throw.v.01\": 74,\n",
      "    \"touch.v.01\": 75,\n",
      "    \"travel.v.01\": 76,\n",
      "    \"turn.v.09\": 77,\n",
      "    \"unknown\": 78,\n",
      "    \"unlock.v.01\": 79,\n",
      "    \"wait.v.01\": 80,\n",
      "    \"wake_up.v.02\": 81,\n",
      "    \"wear.v.02\": 82,\n",
      "    \"write.v.07\": 83\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file vn-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"vn-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file vn-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at vn-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f12a09ae554bd2830008ae3ff3387f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce08691c76ae4d7685ef83c802cf7889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb69ce1faa9346c79b1ec2f1cf5e20a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3138\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 990\n",
      "Trainer is attempting to log a value of \"{0: 'answer.v.01', 1: 'ask.v.01', 2: 'ask.v.02', 3: 'blow.v.01', 4: 'brandish.v.01', 5: 'break.v.05', 6: 'burn.v.01', 7: 'buy.v.01', 8: 'charge.v.17', 9: 'choose.v.01', 10: 'clean.v.01', 11: 'climb.v.01', 12: 'close.v.01', 13: 'connect.v.01', 14: 'consult.v.02', 15: 'cut.v.01', 16: 'dig.v.01', 17: 'drink.v.01', 18: 'drive.v.01', 19: 'drop.v.01', 20: 'eat.v.01', 21: 'enter.v.01', 22: 'examine.v.02', 23: 'exit.v.01', 24: 'fill.v.01', 25: 'follow.v.01', 26: 'give.v.03', 27: 'hit.v.02', 28: 'hit.v.03', 29: 'insert.v.01', 30: 'insert.v.02', 31: 'inventory.v.01', 32: 'jump.v.01', 33: 'kill.v.01', 34: 'lie_down.v.01', 35: 'light_up.v.05', 36: 'listen.v.01', 37: 'look.v.01', 38: 'lower.v.01', 39: 'memorize.v.01', 40: 'move.v.02', 41: 'note.v.04', 42: 'open.v.01', 43: 'play.v.03', 44: 'pour.v.01', 45: 'pray.v.01', 46: 'press.v.01', 47: 'pull.v.04', 48: 'push.v.01', 49: 'put.v.01', 50: 'raise.v.02', 51: 'read.v.01', 52: 'remove.v.01', 53: 'repeat.v.01', 54: 'rub.v.01', 55: 'say.v.08', 56: 'search.v.04', 57: 'sequence.n.02', 58: 'set.v.05', 59: 'shake.v.01', 60: 'shoot.v.01', 61: 'show.v.01', 62: 'sit_down.v.01', 63: 'skid.v.04', 64: 'sleep.v.01', 65: 'smash.v.02', 66: 'smell.v.01', 67: 'stand.v.03', 68: 'switch_off.v.01', 69: 'switch_on.v.01', 70: 'take.v.04', 71: 'take_off.v.06', 72: 'talk.v.02', 73: 'tell.v.03', 74: 'throw.v.01', 75: 'touch.v.01', 76: 'travel.v.01', 77: 'turn.v.09', 78: 'unknown', 79: 'unlock.v.01', 80: 'wait.v.01', 81: 'wake_up.v.02', 82: 'wear.v.02', 83: 'write.v.07'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'answer.v.01': 0, 'ask.v.01': 1, 'ask.v.02': 2, 'blow.v.01': 3, 'brandish.v.01': 4, 'break.v.05': 5, 'burn.v.01': 6, 'buy.v.01': 7, 'charge.v.17': 8, 'choose.v.01': 9, 'clean.v.01': 10, 'climb.v.01': 11, 'close.v.01': 12, 'connect.v.01': 13, 'consult.v.02': 14, 'cut.v.01': 15, 'dig.v.01': 16, 'drink.v.01': 17, 'drive.v.01': 18, 'drop.v.01': 19, 'eat.v.01': 20, 'enter.v.01': 21, 'examine.v.02': 22, 'exit.v.01': 23, 'fill.v.01': 24, 'follow.v.01': 25, 'give.v.03': 26, 'hit.v.02': 27, 'hit.v.03': 28, 'insert.v.01': 29, 'insert.v.02': 30, 'inventory.v.01': 31, 'jump.v.01': 32, 'kill.v.01': 33, 'lie_down.v.01': 34, 'light_up.v.05': 35, 'listen.v.01': 36, 'look.v.01': 37, 'lower.v.01': 38, 'memorize.v.01': 39, 'move.v.02': 40, 'note.v.04': 41, 'open.v.01': 42, 'play.v.03': 43, 'pour.v.01': 44, 'pray.v.01': 45, 'press.v.01': 46, 'pull.v.04': 47, 'push.v.01': 48, 'put.v.01': 49, 'raise.v.02': 50, 'read.v.01': 51, 'remove.v.01': 52, 'repeat.v.01': 53, 'rub.v.01': 54, 'say.v.08': 55, 'search.v.04': 56, 'sequence.n.02': 57, 'set.v.05': 58, 'shake.v.01': 59, 'shoot.v.01': 60, 'show.v.01': 61, 'sit_down.v.01': 62, 'skid.v.04': 63, 'sleep.v.01': 64, 'smash.v.02': 65, 'smell.v.01': 66, 'stand.v.03': 67, 'switch_off.v.01': 68, 'switch_on.v.01': 69, 'take.v.04': 70, 'take_off.v.06': 71, 'talk.v.02': 72, 'tell.v.03': 73, 'throw.v.01': 74, 'touch.v.01': 75, 'travel.v.01': 76, 'turn.v.09': 77, 'unknown': 78, 'unlock.v.01': 79, 'wait.v.01': 80, 'wake_up.v.02': 81, 'wear.v.02': 82, 'write.v.07': 83}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [990/990 01:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.204472</td>\n",
       "      <td>0.473615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.584177</td>\n",
       "      <td>0.612137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.059297</td>\n",
       "      <td>0.766491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.778776</td>\n",
       "      <td>0.845646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.638297</td>\n",
       "      <td>0.869393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.919400</td>\n",
       "      <td>0.541795</td>\n",
       "      <td>0.881266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.919400</td>\n",
       "      <td>0.488972</td>\n",
       "      <td>0.890501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.919400</td>\n",
       "      <td>0.456037</td>\n",
       "      <td>0.907652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.919400</td>\n",
       "      <td>0.433751</td>\n",
       "      <td>0.908971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.919400</td>\n",
       "      <td>0.427066</td>\n",
       "      <td>0.911609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-99\n",
      "Configuration saved in wn-trainer/checkpoint-99/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-99/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-99/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-99/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-198\n",
      "Configuration saved in wn-trainer/checkpoint-198/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-198/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-99] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-297\n",
      "Configuration saved in wn-trainer/checkpoint-297/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-297/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-297/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-297/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-198] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-396\n",
      "Configuration saved in wn-trainer/checkpoint-396/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-396/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-396/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-396/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-297] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-495\n",
      "Configuration saved in wn-trainer/checkpoint-495/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-495/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-495/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-495/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-396] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-594\n",
      "Configuration saved in wn-trainer/checkpoint-594/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-594/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-594/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-594/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-495] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-693\n",
      "Configuration saved in wn-trainer/checkpoint-693/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-693/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-693/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-693/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-594] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-792\n",
      "Configuration saved in wn-trainer/checkpoint-792/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-693] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-891\n",
      "Configuration saved in wn-trainer/checkpoint-891/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-891/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-891/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-891/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-792] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 758\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to wn-trainer/checkpoint-990\n",
      "Configuration saved in wn-trainer/checkpoint-990/config.json\n",
      "Model weights saved in wn-trainer/checkpoint-990/pytorch_model.bin\n",
      "tokenizer config file saved in wn-trainer/checkpoint-990/tokenizer_config.json\n",
      "Special tokens file saved in wn-trainer/checkpoint-990/special_tokens_map.json\n",
      "Deleting older checkpoint [wn-trainer/checkpoint-891] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from wn-trainer/checkpoint-990 (score: 0.42706620693206787).\n",
      "Configuration saved in wn-trainer/config.json\n",
      "Model weights saved in wn-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "run(\"wn\", 84, \"vn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"unknown\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"unknown\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file mnli-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"mnli-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file mnli-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at mnli-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-3f8ca9077ebf7903.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-69861843bd4c23f8.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-b98ff5898fa6ba87/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-419c9d0b80a90715.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4159\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1300' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1300/1300 01:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in npc_full-trainer/config.json\n",
      "Model weights saved in npc_full-trainer/pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"112\",\n",
      "    \"1\": \"1132\",\n",
      "    \"2\": \"1210\",\n",
      "    \"3\": \"1262\",\n",
      "    \"4\": \"139\",\n",
      "    \"5\": \"1390\",\n",
      "    \"6\": \"16\",\n",
      "    \"7\": \"160\",\n",
      "    \"8\": \"1688\",\n",
      "    \"9\": \"17\",\n",
      "    \"10\": \"171\",\n",
      "    \"11\": \"197\",\n",
      "    \"12\": \"2135\",\n",
      "    \"13\": \"2161\",\n",
      "    \"14\": \"254\",\n",
      "    \"15\": \"263\",\n",
      "    \"16\": \"264\",\n",
      "    \"17\": \"265\",\n",
      "    \"18\": \"273\",\n",
      "    \"19\": \"276\",\n",
      "    \"20\": \"279\",\n",
      "    \"21\": \"2824\",\n",
      "    \"22\": \"283\",\n",
      "    \"23\": \"289\",\n",
      "    \"24\": \"293\",\n",
      "    \"25\": \"301\",\n",
      "    \"26\": \"31\",\n",
      "    \"27\": \"352\",\n",
      "    \"28\": \"395\",\n",
      "    \"29\": \"40\",\n",
      "    \"30\": \"41\",\n",
      "    \"31\": \"410\",\n",
      "    \"32\": \"414\",\n",
      "    \"33\": \"416\",\n",
      "    \"34\": \"42\",\n",
      "    \"35\": \"424\",\n",
      "    \"36\": \"43\",\n",
      "    \"37\": \"54\",\n",
      "    \"38\": \"55\",\n",
      "    \"39\": \"56\",\n",
      "    \"40\": \"59\",\n",
      "    \"41\": \"590\",\n",
      "    \"42\": \"6\",\n",
      "    \"43\": \"62\",\n",
      "    \"44\": \"63\",\n",
      "    \"45\": \"64\",\n",
      "    \"46\": \"65\",\n",
      "    \"47\": \"652\",\n",
      "    \"48\": \"66\",\n",
      "    \"49\": \"683\",\n",
      "    \"50\": \"7\",\n",
      "    \"51\": \"770\",\n",
      "    \"52\": \"801\",\n",
      "    \"53\": \"unknown\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"112\": 0,\n",
      "    \"1132\": 1,\n",
      "    \"1210\": 2,\n",
      "    \"1262\": 3,\n",
      "    \"139\": 4,\n",
      "    \"1390\": 5,\n",
      "    \"16\": 6,\n",
      "    \"160\": 7,\n",
      "    \"1688\": 8,\n",
      "    \"17\": 9,\n",
      "    \"171\": 10,\n",
      "    \"197\": 11,\n",
      "    \"2135\": 12,\n",
      "    \"2161\": 13,\n",
      "    \"254\": 14,\n",
      "    \"263\": 15,\n",
      "    \"264\": 16,\n",
      "    \"265\": 17,\n",
      "    \"273\": 18,\n",
      "    \"276\": 19,\n",
      "    \"279\": 20,\n",
      "    \"2824\": 21,\n",
      "    \"283\": 22,\n",
      "    \"289\": 23,\n",
      "    \"293\": 24,\n",
      "    \"301\": 25,\n",
      "    \"31\": 26,\n",
      "    \"352\": 27,\n",
      "    \"395\": 28,\n",
      "    \"40\": 29,\n",
      "    \"41\": 30,\n",
      "    \"410\": 31,\n",
      "    \"414\": 32,\n",
      "    \"416\": 33,\n",
      "    \"42\": 34,\n",
      "    \"424\": 35,\n",
      "    \"43\": 36,\n",
      "    \"54\": 37,\n",
      "    \"55\": 38,\n",
      "    \"56\": 39,\n",
      "    \"59\": 40,\n",
      "    \"590\": 41,\n",
      "    \"6\": 42,\n",
      "    \"62\": 43,\n",
      "    \"63\": 44,\n",
      "    \"64\": 45,\n",
      "    \"65\": 46,\n",
      "    \"652\": 47,\n",
      "    \"66\": 48,\n",
      "    \"683\": 49,\n",
      "    \"7\": 50,\n",
      "    \"770\": 51,\n",
      "    \"801\": 52,\n",
      "    \"unknown\": 53\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file npc_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"npc_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file npc_full-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at npc_full-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-16992819a956effa.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-457048c1295c2d0c.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7f8c5f914cfc28ef/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-8e0a8228dbe6ad41.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4149\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1300\n",
      "Trainer is attempting to log a value of \"{0: '112', 1: '1132', 2: '1210', 3: '1262', 4: '139', 5: '1390', 6: '16', 7: '160', 8: '1688', 9: '17', 10: '171', 11: '197', 12: '2135', 13: '2161', 14: '254', 15: '263', 16: '264', 17: '265', 18: '273', 19: '276', 20: '279', 21: '2824', 22: '283', 23: '289', 24: '293', 25: '301', 26: '31', 27: '352', 28: '395', 29: '40', 30: '41', 31: '410', 32: '414', 33: '416', 34: '42', 35: '424', 36: '43', 37: '54', 38: '55', 39: '56', 40: '59', 41: '590', 42: '6', 43: '62', 44: '63', 45: '64', 46: '65', 47: '652', 48: '66', 49: '683', 50: '7', 51: '770', 52: '801', 53: 'unknown'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'112': 0, '1132': 1, '1210': 2, '1262': 3, '139': 4, '1390': 5, '16': 6, '160': 7, '1688': 8, '17': 9, '171': 10, '197': 11, '2135': 12, '2161': 13, '254': 14, '263': 15, '264': 16, '265': 17, '273': 18, '276': 19, '279': 20, '2824': 21, '283': 22, '289': 23, '293': 24, '301': 25, '31': 26, '352': 27, '395': 28, '40': 29, '41': 30, '410': 31, '414': 32, '416': 33, '42': 34, '424': 35, '43': 36, '54': 37, '55': 38, '56': 39, '59': 40, '590': 41, '6': 42, '62': 43, '63': 44, '64': 45, '65': 46, '652': 47, '66': 48, '683': 49, '7': 50, '770': 51, '801': 52, 'unknown': 53}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1300' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1300/1300 01:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.724200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in fn_full-trainer/config.json\n",
      "Model weights saved in fn_full-trainer/pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"appear-48.1.1\",\n",
      "    \"1\": \"assuming_position-50\",\n",
      "    \"2\": \"banish-10.2\",\n",
      "    \"3\": \"body_internal_states-40.6\",\n",
      "    \"4\": \"break-45.1\",\n",
      "    \"5\": \"bump-18.4\",\n",
      "    \"6\": \"carry-11.4\",\n",
      "    \"7\": \"carry-11.4-1-1\",\n",
      "    \"8\": \"carve-21.2-2\",\n",
      "    \"9\": \"chase-51.6\",\n",
      "    \"10\": \"crane-40.3.2\",\n",
      "    \"11\": \"cut-21.1-1\",\n",
      "    \"12\": \"disassemble-23.3\",\n",
      "    \"13\": \"eat-39.1-1\",\n",
      "    \"14\": \"eat-39.1-2\",\n",
      "    \"15\": \"escape-51.1-1\",\n",
      "    \"16\": \"exist-47.1-1\",\n",
      "    \"17\": \"feeding-39.7\",\n",
      "    \"18\": \"fill-9.8\",\n",
      "    \"19\": \"fulfilling-13.4.1\",\n",
      "    \"20\": \"funnel-9.3-2-1\",\n",
      "    \"21\": \"get-13.5.1\",\n",
      "    \"22\": \"give-13.1-1\",\n",
      "    \"23\": \"hit-18.1-1\",\n",
      "    \"24\": \"hurt-40.8.3-2\",\n",
      "    \"25\": \"investigate-35.4\",\n",
      "    \"26\": \"knead-26.5\",\n",
      "    \"27\": \"learn-14-1\",\n",
      "    \"28\": \"learn-14-2-1\",\n",
      "    \"29\": \"long-32.2-1\",\n",
      "    \"30\": \"manner_speaking-37.3\",\n",
      "    \"31\": \"modes_of_being_with_motion-47.3\",\n",
      "    \"32\": \"murder-42.1-1\",\n",
      "    \"33\": \"other_cos-45.4\",\n",
      "    \"34\": \"peer-30.3\",\n",
      "    \"35\": \"performance-26.7-1-1\",\n",
      "    \"36\": \"pour-9.5\",\n",
      "    \"37\": \"preparing-26.3-2\",\n",
      "    \"38\": \"push-12-1\",\n",
      "    \"39\": \"push-12-1-1\",\n",
      "    \"40\": \"put-9.1-2\",\n",
      "    \"41\": \"put_direction-9.4\",\n",
      "    \"42\": \"put_spatial-9.2-1\",\n",
      "    \"43\": \"remove-10.1\",\n",
      "    \"44\": \"roll-51.3.1\",\n",
      "    \"45\": \"run-51.3.2\",\n",
      "    \"46\": \"say-37.7-1\",\n",
      "    \"47\": \"scribble-25.2\",\n",
      "    \"48\": \"search-35.2\",\n",
      "    \"49\": \"see-30.1\",\n",
      "    \"50\": \"see-30.1-1\",\n",
      "    \"51\": \"shake-22.3-2\",\n",
      "    \"52\": \"sight-30.2\",\n",
      "    \"53\": \"simple_dressing-41.3.1\",\n",
      "    \"54\": \"slide-11.2\",\n",
      "    \"55\": \"slide-11.2-1-1\",\n",
      "    \"56\": \"snooze-40.4\",\n",
      "    \"57\": \"spatial_configuration-47.6\",\n",
      "    \"58\": \"split-23.2\",\n",
      "    \"59\": \"spray-9.7-1\",\n",
      "    \"60\": \"stalk-35.3\",\n",
      "    \"61\": \"substance_emission-43.4\",\n",
      "    \"62\": \"talk-37.5\",\n",
      "    \"63\": \"tape-22.4\",\n",
      "    \"64\": \"throw-17.1-1-1\",\n",
      "    \"65\": \"touch-20-1\",\n",
      "    \"66\": \"transcribe-25.4\",\n",
      "    \"67\": \"transfer_mesg-37.1.1\",\n",
      "    \"68\": \"unknown\",\n",
      "    \"69\": \"wipe_manner-10.4.1-1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"appear-48.1.1\": 0,\n",
      "    \"assuming_position-50\": 1,\n",
      "    \"banish-10.2\": 2,\n",
      "    \"body_internal_states-40.6\": 3,\n",
      "    \"break-45.1\": 4,\n",
      "    \"bump-18.4\": 5,\n",
      "    \"carry-11.4\": 6,\n",
      "    \"carry-11.4-1-1\": 7,\n",
      "    \"carve-21.2-2\": 8,\n",
      "    \"chase-51.6\": 9,\n",
      "    \"crane-40.3.2\": 10,\n",
      "    \"cut-21.1-1\": 11,\n",
      "    \"disassemble-23.3\": 12,\n",
      "    \"eat-39.1-1\": 13,\n",
      "    \"eat-39.1-2\": 14,\n",
      "    \"escape-51.1-1\": 15,\n",
      "    \"exist-47.1-1\": 16,\n",
      "    \"feeding-39.7\": 17,\n",
      "    \"fill-9.8\": 18,\n",
      "    \"fulfilling-13.4.1\": 19,\n",
      "    \"funnel-9.3-2-1\": 20,\n",
      "    \"get-13.5.1\": 21,\n",
      "    \"give-13.1-1\": 22,\n",
      "    \"hit-18.1-1\": 23,\n",
      "    \"hurt-40.8.3-2\": 24,\n",
      "    \"investigate-35.4\": 25,\n",
      "    \"knead-26.5\": 26,\n",
      "    \"learn-14-1\": 27,\n",
      "    \"learn-14-2-1\": 28,\n",
      "    \"long-32.2-1\": 29,\n",
      "    \"manner_speaking-37.3\": 30,\n",
      "    \"modes_of_being_with_motion-47.3\": 31,\n",
      "    \"murder-42.1-1\": 32,\n",
      "    \"other_cos-45.4\": 33,\n",
      "    \"peer-30.3\": 34,\n",
      "    \"performance-26.7-1-1\": 35,\n",
      "    \"pour-9.5\": 36,\n",
      "    \"preparing-26.3-2\": 37,\n",
      "    \"push-12-1\": 38,\n",
      "    \"push-12-1-1\": 39,\n",
      "    \"put-9.1-2\": 40,\n",
      "    \"put_direction-9.4\": 41,\n",
      "    \"put_spatial-9.2-1\": 42,\n",
      "    \"remove-10.1\": 43,\n",
      "    \"roll-51.3.1\": 44,\n",
      "    \"run-51.3.2\": 45,\n",
      "    \"say-37.7-1\": 46,\n",
      "    \"scribble-25.2\": 47,\n",
      "    \"search-35.2\": 48,\n",
      "    \"see-30.1\": 49,\n",
      "    \"see-30.1-1\": 50,\n",
      "    \"shake-22.3-2\": 51,\n",
      "    \"sight-30.2\": 52,\n",
      "    \"simple_dressing-41.3.1\": 53,\n",
      "    \"slide-11.2\": 54,\n",
      "    \"slide-11.2-1-1\": 55,\n",
      "    \"snooze-40.4\": 56,\n",
      "    \"spatial_configuration-47.6\": 57,\n",
      "    \"split-23.2\": 58,\n",
      "    \"spray-9.7-1\": 59,\n",
      "    \"stalk-35.3\": 60,\n",
      "    \"substance_emission-43.4\": 61,\n",
      "    \"talk-37.5\": 62,\n",
      "    \"tape-22.4\": 63,\n",
      "    \"throw-17.1-1-1\": 64,\n",
      "    \"touch-20-1\": 65,\n",
      "    \"transcribe-25.4\": 66,\n",
      "    \"transfer_mesg-37.1.1\": 67,\n",
      "    \"unknown\": 68,\n",
      "    \"wipe_manner-10.4.1-1\": 69\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file fn_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"fn_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file fn_full-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at fn_full-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-e510c9c3f5d66e04.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-df903802313c9255.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-7581f73791510896/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-e5cc2004f8bb6dc5.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5872\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1840\n",
      "Trainer is attempting to log a value of \"{0: 'appear-48.1.1', 1: 'assuming_position-50', 2: 'banish-10.2', 3: 'body_internal_states-40.6', 4: 'break-45.1', 5: 'bump-18.4', 6: 'carry-11.4', 7: 'carry-11.4-1-1', 8: 'carve-21.2-2', 9: 'chase-51.6', 10: 'crane-40.3.2', 11: 'cut-21.1-1', 12: 'disassemble-23.3', 13: 'eat-39.1-1', 14: 'eat-39.1-2', 15: 'escape-51.1-1', 16: 'exist-47.1-1', 17: 'feeding-39.7', 18: 'fill-9.8', 19: 'fulfilling-13.4.1', 20: 'funnel-9.3-2-1', 21: 'get-13.5.1', 22: 'give-13.1-1', 23: 'hit-18.1-1', 24: 'hurt-40.8.3-2', 25: 'investigate-35.4', 26: 'knead-26.5', 27: 'learn-14-1', 28: 'learn-14-2-1', 29: 'long-32.2-1', 30: 'manner_speaking-37.3', 31: 'modes_of_being_with_motion-47.3', 32: 'murder-42.1-1', 33: 'other_cos-45.4', 34: 'peer-30.3', 35: 'performance-26.7-1-1', 36: 'pour-9.5', 37: 'preparing-26.3-2', 38: 'push-12-1', 39: 'push-12-1-1', 40: 'put-9.1-2', 41: 'put_direction-9.4', 42: 'put_spatial-9.2-1', 43: 'remove-10.1', 44: 'roll-51.3.1', 45: 'run-51.3.2', 46: 'say-37.7-1', 47: 'scribble-25.2', 48: 'search-35.2', 49: 'see-30.1', 50: 'see-30.1-1', 51: 'shake-22.3-2', 52: 'sight-30.2', 53: 'simple_dressing-41.3.1', 54: 'slide-11.2', 55: 'slide-11.2-1-1', 56: 'snooze-40.4', 57: 'spatial_configuration-47.6', 58: 'split-23.2', 59: 'spray-9.7-1', 60: 'stalk-35.3', 61: 'substance_emission-43.4', 62: 'talk-37.5', 63: 'tape-22.4', 64: 'throw-17.1-1-1', 65: 'touch-20-1', 66: 'transcribe-25.4', 67: 'transfer_mesg-37.1.1', 68: 'unknown', 69: 'wipe_manner-10.4.1-1'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'appear-48.1.1': 0, 'assuming_position-50': 1, 'banish-10.2': 2, 'body_internal_states-40.6': 3, 'break-45.1': 4, 'bump-18.4': 5, 'carry-11.4': 6, 'carry-11.4-1-1': 7, 'carve-21.2-2': 8, 'chase-51.6': 9, 'crane-40.3.2': 10, 'cut-21.1-1': 11, 'disassemble-23.3': 12, 'eat-39.1-1': 13, 'eat-39.1-2': 14, 'escape-51.1-1': 15, 'exist-47.1-1': 16, 'feeding-39.7': 17, 'fill-9.8': 18, 'fulfilling-13.4.1': 19, 'funnel-9.3-2-1': 20, 'get-13.5.1': 21, 'give-13.1-1': 22, 'hit-18.1-1': 23, 'hurt-40.8.3-2': 24, 'investigate-35.4': 25, 'knead-26.5': 26, 'learn-14-1': 27, 'learn-14-2-1': 28, 'long-32.2-1': 29, 'manner_speaking-37.3': 30, 'modes_of_being_with_motion-47.3': 31, 'murder-42.1-1': 32, 'other_cos-45.4': 33, 'peer-30.3': 34, 'performance-26.7-1-1': 35, 'pour-9.5': 36, 'preparing-26.3-2': 37, 'push-12-1': 38, 'push-12-1-1': 39, 'put-9.1-2': 40, 'put_direction-9.4': 41, 'put_spatial-9.2-1': 42, 'remove-10.1': 43, 'roll-51.3.1': 44, 'run-51.3.2': 45, 'say-37.7-1': 46, 'scribble-25.2': 47, 'search-35.2': 48, 'see-30.1': 49, 'see-30.1-1': 50, 'shake-22.3-2': 51, 'sight-30.2': 52, 'simple_dressing-41.3.1': 53, 'slide-11.2': 54, 'slide-11.2-1-1': 55, 'snooze-40.4': 56, 'spatial_configuration-47.6': 57, 'split-23.2': 58, 'spray-9.7-1': 59, 'stalk-35.3': 60, 'substance_emission-43.4': 61, 'talk-37.5': 62, 'tape-22.4': 63, 'throw-17.1-1-1': 64, 'touch-20-1': 65, 'transcribe-25.4': 66, 'transfer_mesg-37.1.1': 67, 'unknown': 68, 'wipe_manner-10.4.1-1': 69}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1840' max='1840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1840/1840 02:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.853400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in vn_full-trainer/config.json\n",
      "Model weights saved in vn_full-trainer/pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ap_default/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"answer.v.01\",\n",
      "    \"1\": \"ask.v.01\",\n",
      "    \"2\": \"ask.v.02\",\n",
      "    \"3\": \"blow.v.01\",\n",
      "    \"4\": \"brandish.v.01\",\n",
      "    \"5\": \"break.v.05\",\n",
      "    \"6\": \"burn.v.01\",\n",
      "    \"7\": \"buy.v.01\",\n",
      "    \"8\": \"charge.v.17\",\n",
      "    \"9\": \"choose.v.01\",\n",
      "    \"10\": \"clean.v.01\",\n",
      "    \"11\": \"climb.v.01\",\n",
      "    \"12\": \"close.v.01\",\n",
      "    \"13\": \"connect.v.01\",\n",
      "    \"14\": \"consult.v.02\",\n",
      "    \"15\": \"cut.v.01\",\n",
      "    \"16\": \"dig.v.01\",\n",
      "    \"17\": \"drink.v.01\",\n",
      "    \"18\": \"drive.v.01\",\n",
      "    \"19\": \"drop.v.01\",\n",
      "    \"20\": \"eat.v.01\",\n",
      "    \"21\": \"enter.v.01\",\n",
      "    \"22\": \"examine.v.02\",\n",
      "    \"23\": \"exit.v.01\",\n",
      "    \"24\": \"fill.v.01\",\n",
      "    \"25\": \"follow.v.01\",\n",
      "    \"26\": \"give.v.03\",\n",
      "    \"27\": \"hit.v.02\",\n",
      "    \"28\": \"hit.v.03\",\n",
      "    \"29\": \"insert.v.01\",\n",
      "    \"30\": \"insert.v.02\",\n",
      "    \"31\": \"inventory.v.01\",\n",
      "    \"32\": \"jump.v.01\",\n",
      "    \"33\": \"kill.v.01\",\n",
      "    \"34\": \"lie_down.v.01\",\n",
      "    \"35\": \"light_up.v.05\",\n",
      "    \"36\": \"listen.v.01\",\n",
      "    \"37\": \"look.v.01\",\n",
      "    \"38\": \"lower.v.01\",\n",
      "    \"39\": \"memorize.v.01\",\n",
      "    \"40\": \"move.v.02\",\n",
      "    \"41\": \"note.v.04\",\n",
      "    \"42\": \"open.v.01\",\n",
      "    \"43\": \"play.v.03\",\n",
      "    \"44\": \"pour.v.01\",\n",
      "    \"45\": \"pray.v.01\",\n",
      "    \"46\": \"press.v.01\",\n",
      "    \"47\": \"pull.v.04\",\n",
      "    \"48\": \"push.v.01\",\n",
      "    \"49\": \"put.v.01\",\n",
      "    \"50\": \"raise.v.02\",\n",
      "    \"51\": \"read.v.01\",\n",
      "    \"52\": \"remove.v.01\",\n",
      "    \"53\": \"repeat.v.01\",\n",
      "    \"54\": \"rub.v.01\",\n",
      "    \"55\": \"say.v.08\",\n",
      "    \"56\": \"search.v.04\",\n",
      "    \"57\": \"sequence.n.02\",\n",
      "    \"58\": \"set.v.05\",\n",
      "    \"59\": \"shake.v.01\",\n",
      "    \"60\": \"shoot.v.01\",\n",
      "    \"61\": \"show.v.01\",\n",
      "    \"62\": \"sit_down.v.01\",\n",
      "    \"63\": \"skid.v.04\",\n",
      "    \"64\": \"sleep.v.01\",\n",
      "    \"65\": \"smash.v.02\",\n",
      "    \"66\": \"smell.v.01\",\n",
      "    \"67\": \"stand.v.03\",\n",
      "    \"68\": \"switch_off.v.01\",\n",
      "    \"69\": \"switch_on.v.01\",\n",
      "    \"70\": \"take.v.04\",\n",
      "    \"71\": \"take_off.v.06\",\n",
      "    \"72\": \"talk.v.02\",\n",
      "    \"73\": \"tell.v.03\",\n",
      "    \"74\": \"throw.v.01\",\n",
      "    \"75\": \"touch.v.01\",\n",
      "    \"76\": \"travel.v.01\",\n",
      "    \"77\": \"turn.v.09\",\n",
      "    \"78\": \"unknown\",\n",
      "    \"79\": \"unlock.v.01\",\n",
      "    \"80\": \"wait.v.01\",\n",
      "    \"81\": \"wake_up.v.02\",\n",
      "    \"82\": \"wear.v.02\",\n",
      "    \"83\": \"write.v.07\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"answer.v.01\": 0,\n",
      "    \"ask.v.01\": 1,\n",
      "    \"ask.v.02\": 2,\n",
      "    \"blow.v.01\": 3,\n",
      "    \"brandish.v.01\": 4,\n",
      "    \"break.v.05\": 5,\n",
      "    \"burn.v.01\": 6,\n",
      "    \"buy.v.01\": 7,\n",
      "    \"charge.v.17\": 8,\n",
      "    \"choose.v.01\": 9,\n",
      "    \"clean.v.01\": 10,\n",
      "    \"climb.v.01\": 11,\n",
      "    \"close.v.01\": 12,\n",
      "    \"connect.v.01\": 13,\n",
      "    \"consult.v.02\": 14,\n",
      "    \"cut.v.01\": 15,\n",
      "    \"dig.v.01\": 16,\n",
      "    \"drink.v.01\": 17,\n",
      "    \"drive.v.01\": 18,\n",
      "    \"drop.v.01\": 19,\n",
      "    \"eat.v.01\": 20,\n",
      "    \"enter.v.01\": 21,\n",
      "    \"examine.v.02\": 22,\n",
      "    \"exit.v.01\": 23,\n",
      "    \"fill.v.01\": 24,\n",
      "    \"follow.v.01\": 25,\n",
      "    \"give.v.03\": 26,\n",
      "    \"hit.v.02\": 27,\n",
      "    \"hit.v.03\": 28,\n",
      "    \"insert.v.01\": 29,\n",
      "    \"insert.v.02\": 30,\n",
      "    \"inventory.v.01\": 31,\n",
      "    \"jump.v.01\": 32,\n",
      "    \"kill.v.01\": 33,\n",
      "    \"lie_down.v.01\": 34,\n",
      "    \"light_up.v.05\": 35,\n",
      "    \"listen.v.01\": 36,\n",
      "    \"look.v.01\": 37,\n",
      "    \"lower.v.01\": 38,\n",
      "    \"memorize.v.01\": 39,\n",
      "    \"move.v.02\": 40,\n",
      "    \"note.v.04\": 41,\n",
      "    \"open.v.01\": 42,\n",
      "    \"play.v.03\": 43,\n",
      "    \"pour.v.01\": 44,\n",
      "    \"pray.v.01\": 45,\n",
      "    \"press.v.01\": 46,\n",
      "    \"pull.v.04\": 47,\n",
      "    \"push.v.01\": 48,\n",
      "    \"put.v.01\": 49,\n",
      "    \"raise.v.02\": 50,\n",
      "    \"read.v.01\": 51,\n",
      "    \"remove.v.01\": 52,\n",
      "    \"repeat.v.01\": 53,\n",
      "    \"rub.v.01\": 54,\n",
      "    \"say.v.08\": 55,\n",
      "    \"search.v.04\": 56,\n",
      "    \"sequence.n.02\": 57,\n",
      "    \"set.v.05\": 58,\n",
      "    \"shake.v.01\": 59,\n",
      "    \"shoot.v.01\": 60,\n",
      "    \"show.v.01\": 61,\n",
      "    \"sit_down.v.01\": 62,\n",
      "    \"skid.v.04\": 63,\n",
      "    \"sleep.v.01\": 64,\n",
      "    \"smash.v.02\": 65,\n",
      "    \"smell.v.01\": 66,\n",
      "    \"stand.v.03\": 67,\n",
      "    \"switch_off.v.01\": 68,\n",
      "    \"switch_on.v.01\": 69,\n",
      "    \"take.v.04\": 70,\n",
      "    \"take_off.v.06\": 71,\n",
      "    \"talk.v.02\": 72,\n",
      "    \"tell.v.03\": 73,\n",
      "    \"throw.v.01\": 74,\n",
      "    \"touch.v.01\": 75,\n",
      "    \"travel.v.01\": 76,\n",
      "    \"turn.v.09\": 77,\n",
      "    \"unknown\": 78,\n",
      "    \"unlock.v.01\": 79,\n",
      "    \"wait.v.01\": 80,\n",
      "    \"wake_up.v.02\": 81,\n",
      "    \"wear.v.02\": 82,\n",
      "    \"write.v.07\": 83\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/electra-small-discriminator/resolve/main/pytorch_model.bin from cache at /home/ap_default/.cache/huggingface/transformers/1ebdea26ed1a6268cdf5d1fe36450e89c70e306c97d39e62ede8a31f1c43f9ad.baa63624f08a59503441bce3d427225c61fe79bfa9f6d4c30cde7d072d863e0c\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file vn_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"vn_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file vn_full-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at vn_full-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-6cad8b4d87c5490f.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-463188140a8a992b.arrow\n",
      "Loading cached processed dataset at /home/ap_default/.cache/huggingface/datasets/csv/default-dc77ba31d5024efe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-e3010cdeed3fe49c.arrow\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `ElectraForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 3896\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1220\n",
      "Trainer is attempting to log a value of \"{0: 'answer.v.01', 1: 'ask.v.01', 2: 'ask.v.02', 3: 'blow.v.01', 4: 'brandish.v.01', 5: 'break.v.05', 6: 'burn.v.01', 7: 'buy.v.01', 8: 'charge.v.17', 9: 'choose.v.01', 10: 'clean.v.01', 11: 'climb.v.01', 12: 'close.v.01', 13: 'connect.v.01', 14: 'consult.v.02', 15: 'cut.v.01', 16: 'dig.v.01', 17: 'drink.v.01', 18: 'drive.v.01', 19: 'drop.v.01', 20: 'eat.v.01', 21: 'enter.v.01', 22: 'examine.v.02', 23: 'exit.v.01', 24: 'fill.v.01', 25: 'follow.v.01', 26: 'give.v.03', 27: 'hit.v.02', 28: 'hit.v.03', 29: 'insert.v.01', 30: 'insert.v.02', 31: 'inventory.v.01', 32: 'jump.v.01', 33: 'kill.v.01', 34: 'lie_down.v.01', 35: 'light_up.v.05', 36: 'listen.v.01', 37: 'look.v.01', 38: 'lower.v.01', 39: 'memorize.v.01', 40: 'move.v.02', 41: 'note.v.04', 42: 'open.v.01', 43: 'play.v.03', 44: 'pour.v.01', 45: 'pray.v.01', 46: 'press.v.01', 47: 'pull.v.04', 48: 'push.v.01', 49: 'put.v.01', 50: 'raise.v.02', 51: 'read.v.01', 52: 'remove.v.01', 53: 'repeat.v.01', 54: 'rub.v.01', 55: 'say.v.08', 56: 'search.v.04', 57: 'sequence.n.02', 58: 'set.v.05', 59: 'shake.v.01', 60: 'shoot.v.01', 61: 'show.v.01', 62: 'sit_down.v.01', 63: 'skid.v.04', 64: 'sleep.v.01', 65: 'smash.v.02', 66: 'smell.v.01', 67: 'stand.v.03', 68: 'switch_off.v.01', 69: 'switch_on.v.01', 70: 'take.v.04', 71: 'take_off.v.06', 72: 'talk.v.02', 73: 'tell.v.03', 74: 'throw.v.01', 75: 'touch.v.01', 76: 'travel.v.01', 77: 'turn.v.09', 78: 'unknown', 79: 'unlock.v.01', 80: 'wait.v.01', 81: 'wake_up.v.02', 82: 'wear.v.02', 83: 'write.v.07'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'answer.v.01': 0, 'ask.v.01': 1, 'ask.v.02': 2, 'blow.v.01': 3, 'brandish.v.01': 4, 'break.v.05': 5, 'burn.v.01': 6, 'buy.v.01': 7, 'charge.v.17': 8, 'choose.v.01': 9, 'clean.v.01': 10, 'climb.v.01': 11, 'close.v.01': 12, 'connect.v.01': 13, 'consult.v.02': 14, 'cut.v.01': 15, 'dig.v.01': 16, 'drink.v.01': 17, 'drive.v.01': 18, 'drop.v.01': 19, 'eat.v.01': 20, 'enter.v.01': 21, 'examine.v.02': 22, 'exit.v.01': 23, 'fill.v.01': 24, 'follow.v.01': 25, 'give.v.03': 26, 'hit.v.02': 27, 'hit.v.03': 28, 'insert.v.01': 29, 'insert.v.02': 30, 'inventory.v.01': 31, 'jump.v.01': 32, 'kill.v.01': 33, 'lie_down.v.01': 34, 'light_up.v.05': 35, 'listen.v.01': 36, 'look.v.01': 37, 'lower.v.01': 38, 'memorize.v.01': 39, 'move.v.02': 40, 'note.v.04': 41, 'open.v.01': 42, 'play.v.03': 43, 'pour.v.01': 44, 'pray.v.01': 45, 'press.v.01': 46, 'pull.v.04': 47, 'push.v.01': 48, 'put.v.01': 49, 'raise.v.02': 50, 'read.v.01': 51, 'remove.v.01': 52, 'repeat.v.01': 53, 'rub.v.01': 54, 'say.v.08': 55, 'search.v.04': 56, 'sequence.n.02': 57, 'set.v.05': 58, 'shake.v.01': 59, 'shoot.v.01': 60, 'show.v.01': 61, 'sit_down.v.01': 62, 'skid.v.04': 63, 'sleep.v.01': 64, 'smash.v.02': 65, 'smell.v.01': 66, 'stand.v.03': 67, 'switch_off.v.01': 68, 'switch_on.v.01': 69, 'take.v.04': 70, 'take_off.v.06': 71, 'talk.v.02': 72, 'tell.v.03': 73, 'throw.v.01': 74, 'touch.v.01': 75, 'travel.v.01': 76, 'turn.v.09': 77, 'unknown': 78, 'unlock.v.01': 79, 'wait.v.01': 80, 'wake_up.v.02': 81, 'wear.v.02': 82, 'write.v.07': 83}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1220' max='1220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1220/1220 01:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in wn_full-trainer/config.json\n",
      "Model weights saved in wn_full-trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "for task, num_labels, prev in JERICHO_TASKS_FULL:\n",
    "    if task == \"fn_full\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=fn_label2id, id2label=fn_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"npc_full\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=npc_label2id, id2label=npc_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    elif task == \"vn_full\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=vn_label2id, id2label=vn_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, label2id=wn_label2id, id2label=wn_id2label)\n",
    "        if prev is not None:\n",
    "            model.electra = AutoModel.from_pretrained(f\"{prev}-trainer\")\n",
    "\n",
    "    # print(model)\n",
    "    if task == \"fn_full\":\n",
    "        raw_datasets = fn_dataset\n",
    "    elif task == \"npc_full\":\n",
    "        raw_datasets = npc_dataset\n",
    "    elif task == \"vn_full\":\n",
    "        raw_datasets = vn_dataset\n",
    "    else:\n",
    "        raw_datasets = wn_dataset\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    \n",
    "    training_args = TrainingArguments(f\"{task}-trainer\",\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      optim=\"adamw_torch\",\n",
    "                                      learning_rate=1e-4,\n",
    "                                      weight_decay=0.01,\n",
    "                                      warmup_ratio=0.1,\n",
    "                                      adam_epsilon=1e-6,\n",
    "                                      num_train_epochs=10.0,\n",
    "                                      save_strategy=\"no\",\n",
    "                                      evaluation_strategy=\"no\",\n",
    "                                      \n",
    "                                      # Debug\n",
    "                                      #save_steps=2,\n",
    "                                      #eval_steps=2,\n",
    "                                      #max_steps=4,\n",
    "                                      \n",
    "                                      save_total_limit=1,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      per_device_train_batch_size=32,\n",
    "                                      per_device_eval_batch_size=32)\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=tokenized_datasets[\"full\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.model.electra.save_pretrained(f\"{task}-trainer\")\n",
    "    if prev != \"npc_full\" and os.path.exists(f\"{prev}-trainer\"):\n",
    "        try:\n",
    "            shutil.rmtree(f\"{prev}-trainer\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NshqHbIcEXky"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file wn_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"wn_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file wn_full-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at wn_full-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "tokenizer config file saved in if_model/tokenizer_config.json\n",
      "Special tokens file saved in if_model/special_tokens_map.json\n",
      "Configuration saved in if_model/config.json\n",
      "Model weights saved in if_model/pytorch_model.bin\n",
      "loading configuration file wn_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"wn_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file wn_full-trainer/pytorch_model.bin\n",
      "2022-03-30 05:21:40.905784: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 05:21:40.908961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1552 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7\n",
      "Loading PyTorch weights from /home/ap_default/notebooks/electra-if/wn_full-trainer/pytorch_model.bin\n",
      "PyTorch checkpoint contains 13,483,520 parameters\n",
      "Loaded 13,483,008 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "Configuration saved in if_model/config.json\n",
      "Model weights saved in if_model/tf_model.h5\n",
      "loading configuration file npc_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"npc_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file npc_full-trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraModel.\n",
      "\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at npc_full-trainer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "tokenizer config file saved in npc_model/tokenizer_config.json\n",
      "Special tokens file saved in npc_model/special_tokens_map.json\n",
      "Configuration saved in npc_model/config.json\n",
      "Model weights saved in npc_model/pytorch_model.bin\n",
      "loading configuration file npc_full-trainer/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"npc_full-trainer\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file npc_full-trainer/pytorch_model.bin\n",
      "Loading PyTorch weights from /home/ap_default/notebooks/electra-if/npc_full-trainer/pytorch_model.bin\n",
      "PyTorch checkpoint contains 13,483,520 parameters\n",
      "Loaded 13,483,008 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "Configuration saved in npc_model/config.json\n",
      "Model weights saved in npc_model/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "if_model = AutoModel.from_pretrained(\"wn_full-trainer\")\n",
    "tokenizer.save_pretrained(\"if_model\")\n",
    "if_model.save_pretrained(\"if_model\")\n",
    "\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(\"wn_full-trainer\", from_pt=True)\n",
    "tf_model.save_pretrained(\"if_model\")\n",
    "\n",
    "# from transformers import FlaxAutoModel\n",
    "# \n",
    "# fx_model = FlaxAutoModel.from_pretrained(\"wn-trainer\", from_pt=True)\n",
    "# fx_model.save_pretrained(\"if-model\")\n",
    "\n",
    "if_model = AutoModel.from_pretrained(\"npc_full-trainer\")\n",
    "tokenizer.save_pretrained(\"npc_model\")\n",
    "if_model.save_pretrained(\"npc_model\")\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(\"npc_full-trainer\", from_pt=True)\n",
    "tf_model.save_pretrained(\"npc_model\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(\"wn-trainer\")\n",
    "    shutil.rmtree(\"wn_full-trainer\")\n",
    "    shutil.rmtree(\"npc_full-trainer\")\n",
    "    shutil.rmtree(\"mlruns\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XX8qYBIfH3Ip"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: if_model/ (stored 0%)\n",
      "updating: if_model/tokenizer.json (deflated 71%)\n",
      "updating: if_model/special_tokens_map.json (deflated 40%)\n",
      "updating: if_model/tf_model.h5 (deflated 8%)\n",
      "updating: if_model/pytorch_model.bin (deflated 7%)\n",
      "updating: if_model/config.json (deflated 52%)\n",
      "updating: if_model/vocab.txt (deflated 53%)\n",
      "updating: if_model/tokenizer_config.json (deflated 40%)\n",
      "updating: npc_model/ (stored 0%)\n",
      "updating: npc_model/tokenizer.json (deflated 71%)\n",
      "updating: npc_model/special_tokens_map.json (deflated 40%)\n",
      "updating: npc_model/tf_model.h5 (deflated 8%)\n",
      "updating: npc_model/pytorch_model.bin (deflated 7%)\n",
      "updating: npc_model/config.json (deflated 52%)\n",
      "updating: npc_model/vocab.txt (deflated 53%)\n",
      "updating: npc_model/tokenizer_config.json (deflated 40%)\n"
     ]
    }
   ],
   "source": [
    "!zip if_model.zip -r if_model\n",
    "!zip npc_model.zip -r npc_model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "electra_for_interactive_fiction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "conda-env-py38_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "030c493bd2f24cbeac5aff32c84e08ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8436922250a54afe87986720c0744cee",
      "placeholder": "​",
      "style": "IPY_MODEL_9f2aedc08ed4496f84e112337fd456ad",
      "value": " 2/2 [00:00&lt;00:00,  5.92ba/s]"
     }
    },
    "08dfc845c7744d1b92bf1a5f37b505a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d6f43056f4141eda307c272ce35f67d",
       "IPY_MODEL_ec219ab860d347c1b40550486a9a7d39",
       "IPY_MODEL_167ad294664f4ebda553dcaaf5bb5b10"
      ],
      "layout": "IPY_MODEL_665d89b7c97542c1bc4b60fdb8379064"
     }
    },
    "0e5f745e6b9d4a688206ddadcf23642d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "100f5feca8674f529435216ddbd4e822": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12c744fa32af4c459414acaf2f55e9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c971a9fc375d408e9883522e910a174c",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_141e43316cb7494190cc49f240a79862",
      "value": 3
     }
    },
    "141e43316cb7494190cc49f240a79862": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16353e68f4424fb08254ba689b9f66e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "167ad294664f4ebda553dcaaf5bb5b10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c4ebef655e44aebb551406d925d65f1",
      "placeholder": "​",
      "style": "IPY_MODEL_422ab5565f284667bad486a4e4ffd9e0",
      "value": " 2/2 [00:00&lt;00:00, 36.83it/s]"
     }
    },
    "1980a0e16a0b46798730c9ed2f1fa11b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9381e325f0b84f3d8e891668d2ef67ce",
      "placeholder": "​",
      "style": "IPY_MODEL_16353e68f4424fb08254ba689b9f66e1",
      "value": " 2/2 [00:00&lt;00:00, 30.39it/s]"
     }
    },
    "1994343f97164d5db1469b12acef42d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ad0f0bffaa8457a8f89ee3f591a671e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b620978aea943f59eeb76b43fef49a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8b0f35d26fc456085547393bdbb0f18",
      "placeholder": "​",
      "style": "IPY_MODEL_a5ed13c17ec14b799dda4531449a236c",
      "value": " 12/12 [00:10&lt;00:00,  1.25ba/s]"
     }
    },
    "1c05f629e4b347dcb75a361fdd4827e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45f1ba0298cb48c5b576966fe35f76f3",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40b35b96bd9543d38b32cc058f4fc993",
      "value": 2
     }
    },
    "1d03571ce8324a0da4a0149469b6259c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b26b90f6b8a54a8b9ef933f76fdc8b35",
       "IPY_MODEL_8ffe3e0def1040bd8154831e7fb34896",
       "IPY_MODEL_c14fa0725cc14f358d176598b48a0109"
      ],
      "layout": "IPY_MODEL_e14b98564eff4edc81563035d7ec60ec"
     }
    },
    "1d6f43056f4141eda307c272ce35f67d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ad0f0bffaa8457a8f89ee3f591a671e",
      "placeholder": "​",
      "style": "IPY_MODEL_7f873ce8f5f847c6876f3aba0114fd14",
      "value": "100%"
     }
    },
    "20ade413d1a24164a7c5938cc196a156": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2192c7a7132244eba01c1de17e02c3a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21e974b84b7e4fc09e78f1ab9894966f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24f3285bc54c477096767941796a4a43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2518bc84617042e890c1300763303fca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f134a6930640f8b4ab4e721c364943": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "269d1c6df1cd4c19bd19e9a5adebd9f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e16f4c2b7a746f6a434ec955ea32642": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e503116470c4b13825304cd7fe0b422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21e974b84b7e4fc09e78f1ab9894966f",
      "placeholder": "​",
      "style": "IPY_MODEL_424ca82295f44fd19e7b87f7cb82dd81",
      "value": " 9/9 [00:00&lt;00:00, 15.19ba/s]"
     }
    },
    "3490cfd5cd7c4b22be6af705a654ab95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34f8c7f0f59749619fb150c93a30c747": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3638ad82a3704bf895c76102944656bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2526db6b3184dd8b3a4f54e4b722e4e",
      "placeholder": "​",
      "style": "IPY_MODEL_6b2683ee20b2465da6a8691a9de98c28",
      "value": " 2/2 [00:00&lt;00:00, 25.82it/s]"
     }
    },
    "3b6472c99b7a4f20b1bcaa3269982011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a0557e54db44c9798635ac6bd5b9c76",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_898440a4b85140a98afdf358288b9906",
      "value": 9
     }
    },
    "3c43bec5cd074c19ba8e6aa23d73ddbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eeab2c2b2ce1498984b2d101fc21981c",
       "IPY_MODEL_3b6472c99b7a4f20b1bcaa3269982011",
       "IPY_MODEL_2e503116470c4b13825304cd7fe0b422"
      ],
      "layout": "IPY_MODEL_100f5feca8674f529435216ddbd4e822"
     }
    },
    "3cda73a4ed074b898941cf52849551de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0e090bc046c4457a9c08b11a5fb7e64",
      "placeholder": "​",
      "style": "IPY_MODEL_4bb34f0b1cdd41a4bf4d4519704de094",
      "value": "100%"
     }
    },
    "40b35b96bd9543d38b32cc058f4fc993": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40b835a49f214e0daf4ac7ae54b6cb11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c012505f502c4a8a945fb687d7df3a09",
       "IPY_MODEL_12c744fa32af4c459414acaf2f55e9f7",
       "IPY_MODEL_a54a947a944c4fe4bb0fd9270085aa96"
      ],
      "layout": "IPY_MODEL_a1b1ede4f6974900a6bf4c7d86b14750"
     }
    },
    "422ab5565f284667bad486a4e4ffd9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "424ca82295f44fd19e7b87f7cb82dd81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43365b4b2ccc4fd08262f7c30e4a82b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5aaaca4b50574a7f9ad716836c7af2d7",
       "IPY_MODEL_eabe7a29e1a445e2924e04e695fa4a2e",
       "IPY_MODEL_030c493bd2f24cbeac5aff32c84e08ab"
      ],
      "layout": "IPY_MODEL_4c69c3394f7846aaa88942863c2e9533"
     }
    },
    "43d196dfae1344eda078adb9b4cbb4a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45df48cc8cad4614a4d3a7fcaa6166fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f1ba0298cb48c5b576966fe35f76f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48c34e464e6e488b972f8445dcf175cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49546cb3476f4b1a8ad5b08e73f4e18c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bb34f0b1cdd41a4bf4d4519704de094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c69c3394f7846aaa88942863c2e9533": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ea509409ae24c83ad484774746ea1da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51e17e3b39524516950c7473ed5a353f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "546d2c63c5aa4cfe8fa68d13afeaae1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5701d76e624549d5a4c7009444b36676": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75f1ead51877488589d4357feaee2c9c",
       "IPY_MODEL_abde2728613741bcac502f6e573fc78f",
       "IPY_MODEL_1b620978aea943f59eeb76b43fef49a1"
      ],
      "layout": "IPY_MODEL_72e6bac9a1aa4be6a6449a7561555821"
     }
    },
    "58dc4e9220f44a3083f7a282b5f47435": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0651e457e794fffa8361b94b9a53848",
      "placeholder": "​",
      "style": "IPY_MODEL_9d769c1fa6b14115863fc93cda8331e1",
      "value": "100%"
     }
    },
    "5aaaca4b50574a7f9ad716836c7af2d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45df48cc8cad4614a4d3a7fcaa6166fb",
      "placeholder": "​",
      "style": "IPY_MODEL_f9883302291d4455b692ece7e671f375",
      "value": "100%"
     }
    },
    "659fd1af497a4a15afa1caf9a7073b04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "665d89b7c97542c1bc4b60fdb8379064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a49eff8c3a14666b80d38bc2ec3cd09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b2683ee20b2465da6a8691a9de98c28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b320fd80314426a9b27ad21cd51c62e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8eb713e60d64efc988c86f5238a4357",
      "placeholder": "​",
      "style": "IPY_MODEL_8d8927026c544c378a5b1c21e147ae95",
      "value": "100%"
     }
    },
    "6ce213b40883430595cab8f30f5552cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e97b0404fa14d919fe5ff97a62104f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7046b07581974b0cb9e860d55b5bdc7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72e6bac9a1aa4be6a6449a7561555821": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75f1ead51877488589d4357feaee2c9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7046b07581974b0cb9e860d55b5bdc7a",
      "placeholder": "​",
      "style": "IPY_MODEL_97551b6e0b974831a82131fb00c7b385",
      "value": "100%"
     }
    },
    "77e3bda067474890aba63e1d001ec42c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7b29986357944df1bf80b25fdf3e3508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2907cabe1454e068a522fd4e4dab891",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cb679855b374ff9847606cea3c83f43",
      "value": 3
     }
    },
    "7cb679855b374ff9847606cea3c83f43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ee3137f83eb495aa4c11f919007bd58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f873ce8f5f847c6876f3aba0114fd14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "806417c150b8424490d6986db897c3d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8194cf85ee8c4600a0a8cd34a7b8957b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34f8c7f0f59749619fb150c93a30c747",
      "placeholder": "​",
      "style": "IPY_MODEL_bfdb35e2e779443eacb8a54e4e7820a4",
      "value": "100%"
     }
    },
    "8436922250a54afe87986720c0744cee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84886deb6941402b91c91bc04d958bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8643b9dc737f48c49eee8ba632572d1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8194cf85ee8c4600a0a8cd34a7b8957b",
       "IPY_MODEL_b5e93f47b86b47438d372b7552eb17e4",
       "IPY_MODEL_a2665d0eeb9d4bbcb49244b248e80d1d"
      ],
      "layout": "IPY_MODEL_b1a85c24536e48a8a55f591106294ef7"
     }
    },
    "8815ee5211094480bf1f550ea9b8d523": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "898440a4b85140a98afdf358288b9906": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8c4ebef655e44aebb551406d925d65f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d8927026c544c378a5b1c21e147ae95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ddfb3c21db64e3a8edb1a8db84d1172": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c53a9841a2024db6b5c31d12407fed46",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e97b0404fa14d919fe5ff97a62104f8",
      "value": 2
     }
    },
    "8ffe3e0def1040bd8154831e7fb34896": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48c34e464e6e488b972f8445dcf175cf",
      "max": 131,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be812d748b0d436aa4835bda48501d8c",
      "value": 131
     }
    },
    "9381e325f0b84f3d8e891668d2ef67ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "938fe1a7361840fabdcbb224650f4d42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94b015044199410ca35b2478fba4fc7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97551b6e0b974831a82131fb00c7b385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9868181ac7f04b23b41f10b38f8247eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a0557e54db44c9798635ac6bd5b9c76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d769c1fa6b14115863fc93cda8331e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f2aedc08ed4496f84e112337fd456ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1b1ede4f6974900a6bf4c7d86b14750": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2665d0eeb9d4bbcb49244b248e80d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ce213b40883430595cab8f30f5552cb",
      "placeholder": "​",
      "style": "IPY_MODEL_546d2c63c5aa4cfe8fa68d13afeaae1a",
      "value": " 2/2 [00:00&lt;00:00, 32.08it/s]"
     }
    },
    "a523092b45714841945dc3348717ac80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_269d1c6df1cd4c19bd19e9a5adebd9f5",
      "placeholder": "​",
      "style": "IPY_MODEL_f1952f94986446f1907adf7cf750e294",
      "value": " 2/2 [00:00&lt;00:00, 40.74it/s]"
     }
    },
    "a54a947a944c4fe4bb0fd9270085aa96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e16f4c2b7a746f6a434ec955ea32642",
      "placeholder": "​",
      "style": "IPY_MODEL_84886deb6941402b91c91bc04d958bab",
      "value": " 3/3 [00:00&lt;00:00, 52.06it/s]"
     }
    },
    "a5ed13c17ec14b799dda4531449a236c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8294f4700464990ade651c2aa4b26ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a87a8bda98034f12bacd371f684adf3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_938fe1a7361840fabdcbb224650f4d42",
      "placeholder": "​",
      "style": "IPY_MODEL_49546cb3476f4b1a8ad5b08e73f4e18c",
      "value": " 2/2 [00:00&lt;00:00,  5.05ba/s]"
     }
    },
    "a9bc01ab08264b408304d905b0430d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b320fd80314426a9b27ad21cd51c62e",
       "IPY_MODEL_eaaa725557194e31935b84caae659454",
       "IPY_MODEL_3638ad82a3704bf895c76102944656bd"
      ],
      "layout": "IPY_MODEL_659fd1af497a4a15afa1caf9a7073b04"
     }
    },
    "aac2139debf34899ac3d922c1f75844b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7b6d8f6a01f4f78bac63de8cdf2e27e",
       "IPY_MODEL_7b29986357944df1bf80b25fdf3e3508",
       "IPY_MODEL_b8c3222ec9dc4672bbf33f090784a44b"
      ],
      "layout": "IPY_MODEL_3490cfd5cd7c4b22be6af705a654ab95"
     }
    },
    "abde2728613741bcac502f6e573fc78f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20ade413d1a24164a7c5938cc196a156",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94b015044199410ca35b2478fba4fc7f",
      "value": 12
     }
    },
    "b0e090bc046c4457a9c08b11a5fb7e64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1a85c24536e48a8a55f591106294ef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b26b90f6b8a54a8b9ef933f76fdc8b35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ea509409ae24c83ad484774746ea1da",
      "placeholder": "​",
      "style": "IPY_MODEL_e8419492d48749b992f4618555590ab8",
      "value": "100%"
     }
    },
    "b2907cabe1454e068a522fd4e4dab891": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b48ff84adb6e4395bf4e308500033992": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eba5a88daf924767a271a8d5354403e0",
       "IPY_MODEL_dc1bcb2b2e42451bbfe221c650d51d22",
       "IPY_MODEL_a523092b45714841945dc3348717ac80"
      ],
      "layout": "IPY_MODEL_43d196dfae1344eda078adb9b4cbb4a0"
     }
    },
    "b5d7e554e0284c0d9b6bc5001314cfd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5e93f47b86b47438d372b7552eb17e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9868181ac7f04b23b41f10b38f8247eb",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77e3bda067474890aba63e1d001ec42c",
      "value": 2
     }
    },
    "b8c3222ec9dc4672bbf33f090784a44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2192c7a7132244eba01c1de17e02c3a6",
      "placeholder": "​",
      "style": "IPY_MODEL_c1f5cab961dd44a0b29e1000665c3d5d",
      "value": " 3/3 [00:00&lt;00:00,  8.93it/s]"
     }
    },
    "b8cdf9966e4e49aca0c69bcc3c13ea04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bd6a5290ba484bcca0ec681780bb8ae1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3cda73a4ed074b898941cf52849551de",
       "IPY_MODEL_1c05f629e4b347dcb75a361fdd4827e4",
       "IPY_MODEL_a87a8bda98034f12bacd371f684adf3f"
      ],
      "layout": "IPY_MODEL_7ee3137f83eb495aa4c11f919007bd58"
     }
    },
    "be812d748b0d436aa4835bda48501d8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bfdb35e2e779443eacb8a54e4e7820a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c012505f502c4a8a945fb687d7df3a09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a49eff8c3a14666b80d38bc2ec3cd09",
      "placeholder": "​",
      "style": "IPY_MODEL_d5e1493257224bf799d0f73c3cb469b5",
      "value": "100%"
     }
    },
    "c14fa0725cc14f358d176598b48a0109": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e5f745e6b9d4a688206ddadcf23642d",
      "placeholder": "​",
      "style": "IPY_MODEL_f8e2f2823bc04aa792255f0b9bff4be7",
      "value": " 131/131 [01:58&lt;00:00,  1.44ba/s]"
     }
    },
    "c1f5cab961dd44a0b29e1000665c3d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2526db6b3184dd8b3a4f54e4b722e4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c44d998062114815b74977b4e69ff2dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4ce0cae78884d7299d8b7853657c74f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c53a9841a2024db6b5c31d12407fed46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c971a9fc375d408e9883522e910a174c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0651e457e794fffa8361b94b9a53848": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5e1493257224bf799d0f73c3cb469b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d90923208f7446b7b71715ce4f80b924": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc1bcb2b2e42451bbfe221c650d51d22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_806417c150b8424490d6986db897c3d5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1994343f97164d5db1469b12acef42d1",
      "value": 2
     }
    },
    "dfda8acabf3547aba8dc04e56d04c24a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e14b98564eff4edc81563035d7ec60ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8419492d48749b992f4618555590ab8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eaaa725557194e31935b84caae659454": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8815ee5211094480bf1f550ea9b8d523",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8cdf9966e4e49aca0c69bcc3c13ea04",
      "value": 2
     }
    },
    "eabe7a29e1a445e2924e04e695fa4a2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5d7e554e0284c0d9b6bc5001314cfd8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51e17e3b39524516950c7473ed5a353f",
      "value": 2
     }
    },
    "eba5a88daf924767a271a8d5354403e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25f134a6930640f8b4ab4e721c364943",
      "placeholder": "​",
      "style": "IPY_MODEL_c44d998062114815b74977b4e69ff2dc",
      "value": "100%"
     }
    },
    "ec219ab860d347c1b40550486a9a7d39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24f3285bc54c477096767941796a4a43",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dfda8acabf3547aba8dc04e56d04c24a",
      "value": 2
     }
    },
    "eeab2c2b2ce1498984b2d101fc21981c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eed87323b88247998fbbfa177324da90",
      "placeholder": "​",
      "style": "IPY_MODEL_a8294f4700464990ade651c2aa4b26ad",
      "value": "100%"
     }
    },
    "eed87323b88247998fbbfa177324da90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1952f94986446f1907adf7cf750e294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4e47c6f6d704b24b1d5fb26dfce9943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58dc4e9220f44a3083f7a282b5f47435",
       "IPY_MODEL_8ddfb3c21db64e3a8edb1a8db84d1172",
       "IPY_MODEL_1980a0e16a0b46798730c9ed2f1fa11b"
      ],
      "layout": "IPY_MODEL_c4ce0cae78884d7299d8b7853657c74f"
     }
    },
    "f7b6d8f6a01f4f78bac63de8cdf2e27e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2518bc84617042e890c1300763303fca",
      "placeholder": "​",
      "style": "IPY_MODEL_d90923208f7446b7b71715ce4f80b924",
      "value": "100%"
     }
    },
    "f8b0f35d26fc456085547393bdbb0f18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8e2f2823bc04aa792255f0b9bff4be7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8eb713e60d64efc988c86f5238a4357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9883302291d4455b692ece7e671f375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
